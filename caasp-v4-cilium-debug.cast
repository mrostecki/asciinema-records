{"version": 2, "width": 213, "height": 51, "timestamp": 1571224208, "env": {"SHELL": "/bin/bash", "TERM": "xterm-256color"}}
[0.53746, "o", "\u001b]0;mrostecki@linux-hl7a:~\u001b\\\u001b]7;file://linux-hl7a/home/mrostecki\u001b\\"]
[0.539571, "o", "\u001b[0;38;5;231;48;5;31;1m mrostecki \u001b[0;38;5;31;48;5;240;22m \u001b[0;38;5;252;48;5;240;1m~ \u001b[0;38;5;240;49;22m \u001b[0m"]
[1.772898, "o", "\r(reverse-i-search)`': "]
[2.049881, "o", "\b\b\bs': ssh sles@10.86.3.243\b\b\b\b\b\b\b\b\b\b\b\b\b"]
[2.204934, "o", "\b\b\b\b\b\b\b\b\b\b\u001b[1@l\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C"]
[2.304606, "o", "\b\b\b\b\b\b\b\u001b[1@e\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C"]
[2.549572, "o", "\b\b\b\b\b\b\b\u001b[1@s\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C\u001b[C"]
[2.955631, "o", "\r\u001b[9P\u001b[0;38;5;231;48;5;31;1m mrostecki \u001b[0;38;5;31;48;5;240;22m \u001b[0;38;5;252;48;5;240;1m~ \u001b[0;38;5;240;49;22m \u001b[0m\u001b[C\u001b[C\u001b[C\u001b[C\r\n"]
[6.23766, "o", "Last login: Wed Oct 16 11:06:50 2019 from 10.163.2.71\r\r\n"]
[6.490536, "o", "sles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[7.688311, "o", "t"]
[7.779685, "o", "m"]
[7.999057, "o", "u"]
[8.185898, "o", "x"]
[8.859373, "o", "\r\n"]
[8.882288, "o", "\u001b[?1049h\u001b[22;0;0t\u001b[?1h\u001b=\u001b[H\u001b[2J\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1002l\u001b[?1006l\u001b[?1005l\u001b[c\u001b(B\u001b[m\u001b[?12;25h\u001b[?12l\u001b[?25h\u001b[?1003l\u001b[?1006l\u001b[?2004l\u001b[1;1H\u001b[1;51r\u001b]112\u0007\u001b[1;1H"]
[8.886307, "o", "\u001b[?25l\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:10 16-Oct-19\u001b(B\u001b[m\u001b[1;1H\u001b[?12l\u001b[?25h"]
[8.889961, "o", "\u001b(B\u001b[m\u001b[?12;25h\u001b[?12l\u001b[?25h\u001b[?1003l\u001b[?1006l\u001b[?2004l\u001b[1;1H\u001b[1;51r\u001b[1;1H"]
[8.890584, "o", "\u001b[?25l\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:10 16-Oct-19\u001b(B\u001b[m\u001b[1;1H\u001b[?12l\u001b[?25h"]
[9.066498, "o", "sles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[12.53749, "o", "\u001b[H\u001b[25P(reverse-i-search)`':\u001b[C"]
[12.778733, "o", "\u001b[3D-': sudo zypper ar -r http://download.suse.de/ibs/Devel:/PubCloud:/CI:/SLE-15-SP1/SLE_15_SP1/Devel:PubCloud:CI:SLE-15-SP1.repo\u001b[9D"]
[13.081971, "o", "\u001b[21G\u001b[68Po': kubectl -n kube-system get pods -o wide | grep cilium\u001b[21D"]
[13.479903, "o", "\u001b[22G\u001b[1@ \u001b[35C"]
[13.996027, "o", "\u001b[23G\u001b[1@w\u001b[35C"]
[14.105523, "o", "\u001b[24G\u001b[1@i\u001b[35C"]
[14.304527, "o", "\u001b[25G\u001b[1@d\u001b[35C"]
[14.858779, "o", "\u001b[H\u001b[19@sles@caasp-master-mrostecki-caasp-cluster-0:~>\r\n"]
[15.515022, "o", "\u001b[31m\u001b[1mcilium\u001b(B\u001b[m\u001b[K-2ztnx                                 1/1     Running            7          41d   172.28.0.8     my-worker-3          <none>           <none>\u001b[?25l\u001b[51;1H\u001b[30m\u001b[42m[0] 0:kubectl*                                                                                                                                                                \"caasp-master-mrosteck\" 11:10 16-Oct-19\u001b(B\u001b[m\u001b[3;1H\u001b[?12l\u001b[?25h"]
[15.515648, "o", "\u001b[31m\u001b[1mcilium\u001b(B\u001b[m\u001b[K-gq6w7                                 1/1     Running            8          41d   172.28.0.15    mrostecki-master-2   <none>           <none>\r\n"]
[15.516068, "o", "\u001b[31m\u001b[1mcilium\u001b(B\u001b[m\u001b[K-l6wx6                                 1/1     Running            8          41d   172.28.0.31    mrostecki-master-1   <none>           <none>\r\n"]
[15.517034, "o", "\u001b[31m\u001b[1mcilium\u001b(B\u001b[m\u001b[K-operator-585f97b879-57j98             1/1     Running            0          24d   10.244.3.27    my-worker-1          <none>           <none>\r\n"]
[15.517512, "o", "\u001b[31m\u001b[1mcilium\u001b(B\u001b[m\u001b[K-qk8jh                                 1/1     Running            6          39d   172.28.0.18    mrostecki-master-3   <none>           <none>\r\n"]
[15.517826, "o", "\u001b[31m\u001b[1mcilium\u001b(B\u001b[m\u001b[K-sshj7                                 1/1     Running            7          41d   172.28.0.12    my-worker-2          <none>           <none>\r\n"]
[15.518603, "o", "\u001b[31m\u001b[1mcilium\u001b(B\u001b[m\u001b[K-w4wtg                                 1/1     Running            7          41d   172.28.0.21    my-worker-1          <none>           <none>\r\n"]
[15.536973, "o", "sles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[18.702448, "o", "\u001b[?25l\u001b[51;1H\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:10 16-Oct-19\u001b(B\u001b[m\u001b[9;48H\u001b[?12l\u001b[?25h"]
[18.703757, "o", "\r\u001b[25P(reverse-i-search)`':\u001b[C"]
[19.445826, "o", "\u001b[3Dl': kubectl -n kube-system get pods -o wide | grep cilium\u001b[4D"]
[19.638081, "o", "\u001b[21Go': sudo zypper ar -r http://download.suse.de/ibs/Devel:/PubCloud:/CI:/SLE-15-SP1/SLE_15_SP1/Devel:PubCloud:CI:SLE-15-SP1.repo\u001b[23D"]
[19.734789, "o", "\u001b[22G\u001b[78Pg': kubectl -n kube-system logs -f cilium-2ztnx\u001b[20D"]
[20.012905, "o", "\u001b[23G\u001b[1@s\u001b[26C"]
[22.070128, "o", "\r\u001b[21@sles@caasp-master-mrostecki-caasp-cluster-0:~>\r\n"]
[22.739647, "o", "level=info msg=\"Skipped reading configuration file\" reason=\"Config File \\\"ciliumd\\\" Not Found in \\\"[/root]\\\"\" subsys=daemon\r\nlevel=info msg=\"  --access-log=''\" subsys=daemon\r\nlevel=info msg=\"  --agent-labels=''\" subsys=daemon\r\nlevel=info msg=\"  --allow-localhost='auto'\" subsys=daemon\r\nlevel=info msg=\"  --auto-direct-node-routes='false'\" subsys=daemon\r\nlevel=info msg=\"  --bpf-compile-debug='false'\" subsys=daemon\r\nlevel=info msg=\"  --bpf-ct-global-any-max='262144'\" subsys=daemon\r\nlevel=info msg=\"  --bpf-ct-global-tcp-max='1000000'\" subsys=daemon\r\nlevel=info msg=\"  --bpf-policy-map-max='16384'\" subsys=daemon"]
[22.740277, "o", "\u001b[?25l\u001b[51;1H\u001b[30m\u001b[42m[0] 0:kubectl*                                                                                                                                                                \"caasp-master-mrosteck\" 11:10 16-Oct-19\u001b(B\u001b[m\u001b[19;1H\u001b[?12l\u001b[?25h"]
[22.744208, "o", "level=info msg=\"  --bpf-root=''\" subsys=daemon\r\nlevel=info msg=\"  --cgroup-root=''\" subsys=daemon\r\nlevel=info msg=\"  --cluster-id='0'\" subsys=daemon\r\nlevel=info msg=\"  --cluster-name='default'\" subsys=daemon\r\nlevel=info msg=\"  --clustermesh-config=''\" subsys=daemon\r\nlevel=info msg=\"  --cmdref=''\" subsys=daemon\r\nlevel=info msg=\"  --config=''\" subsys=daemon\r\nlevel=info msg=\"  --config-dir=''\" subsys=daemon\r\nlevel=info msg=\"  --conntrack-garbage-collector-interval='0'\" subsys=daemon\r\nlevel=info msg=\"  --conntrack-gc-interval='0s'\" subsys=daemon\r\nlevel=info msg=\"  --container-runtime=''\" subsys=daemon\r\nlevel=info msg=\"  --container-runtime-endpoint='map[]'\" subsys=daemon\r\nlevel=info msg=\"  --datapath-mode='veth'\" subsys=daemon\r\nlevel=info msg=\"  --debug='false'\" subsys=daemon\r\nlevel=info msg=\"  --debug-verbose=''\" subsys=daemon\r\nlevel=info msg=\"  --device='undefined'\" subsys=daemon\r\nlevel=info msg=\"  --disable-conntrack='false'\" subsys=daemon\r\nlevel=info msg=\"  --disable-endpoint-crd='false'\" subsys=daemon\r\nlevel"]
[22.744702, "o", "=info msg=\"  --disable-envoy-version-check='true'\" subsys=daemon\r\nlevel=info msg=\"  --disable-ipv4='false'\" subsys=daemon\r\nlevel=info msg=\"  --disable-k8s-services='false'\" subsys=daemon\r\nlevel=info msg=\"  --docker='unix:///var/run/docker.sock'\" subsys=daemon\r\nlevel=info msg=\"  --enable-health-checking='true'\" subsys=daemon\r\nlevel=info msg=\"  --enable-ipsec='false'\" subsys=daemon\r\nlevel=info msg=\"  --enable-ipv4='true'\" subsys=daemon\r\nlevel=info msg=\"  --enable-ipv6='true'\" subsys=daemon\r\nlevel=info msg=\"  --enable-k8s-event-handover='false'\" subsys=daemon\r\nlevel=info msg=\"  --enable-legacy-services='true'\" subsys=daemon\r\n\u001b[1;50r\u001b[50;213H\n\u001b[46;1Hlevel=info msg=\"  --enable-policy='default'\" subsys=daemon\r\nlevel=info msg=\"  --enable-selective-regeneration='true'\" subsys=daemon\r\nlevel=info msg=\"  --enable-tracing='false'\" subsys=daemon\r\nlevel=info msg=\"  --encrypt-interface=''\" subsys=daemon\u001b[1;51r\u001b[50;1H"]
[22.755585, "o", "\u001b[1;50r\u001b[31S\u001b[19;1Hlevel=info msg=\"  --endpoint-queue-size='25'\" subsys=daemon\r\nlevel=info msg=\"  --envoy-log=''\" subsys=daemon\r\nlevel=info msg=\"  --fixed-identity-mapping='map[]'\" subsys=daemon\r\nlevel=info msg=\"  --flannel-manage-existing-containers='false'\" subsys=daemon\r\nlevel=info msg=\"  --flannel-master-device=''\" subsys=daemon\r\nlevel=info msg=\"  --flannel-uninstall-on-exit='false'\" subsys=daemon\r\nlevel=info msg=\"  --http-403-msg=''\" subsys=daemon\r\nlevel=info msg=\"  --http-idle-timeout='0'\" subsys=daemon\r\nlevel=info msg=\"  --http-max-grpc-timeout='0'\" subsys=daemon\r\nlevel=info msg=\"  --http-request-timeout='3600'\" subsys=daemon\r\nlevel=info msg=\"  --http-retry-count='3'\" subsys=daemon\r\nlevel=info msg=\"  --http-retry-timeout='0'\" subsys=daemon\r\nlevel=info msg=\"  --identity-change-grace-period='5s'\" subsys=daemon\r\nlevel=info msg=\"  --install-iptables-rules='true'\" subsys=daemon\r\nlevel=info msg=\"  --ipv4-cluster-cidr-mask-size='8'\" subsys=daemon\r\nlevel=info msg=\"  --ipv4-node='auto'\" subsys=daemon\r\nlevel=inf"]
[22.756331, "o", "o msg=\"  --ipv4-range='auto'\" subsys=daemon\r\nlevel=info msg=\"  --ipv4-service-range='auto'\" subsys=daemon\r\nlevel=info msg=\"  --ipv6-cluster-alloc-cidr='f00d::/64'\" subsys=daemon\r\nlevel=info msg=\"  --ipv6-node='auto'\" subsys=daemon\r\nlevel=info msg=\"  --ipv6-range='auto'\" subsys=daemon\r\nlevel=info msg=\"  --ipv6-service-range='auto'\" subsys=daemon\r\nlevel=info msg=\"  --ipvlan-master-device='undefined'\" subsys=daemon\r\nlevel=info msg=\"  --k8s-api-server=''\" subsys=daemon\r\nlevel=info msg=\"  --k8s-force-json-patch='false'\" subsys=daemon\r\nlevel=info msg=\"  --k8s-kubeconfig-path=''\" subsys=daemon\r\nlevel=info msg=\"  --k8s-namespace=''\" subsys=daemon\r\nlevel=info msg=\"  --k8s-require-ipv4-pod-cidr='false'\" subsys=daemon\r\nlevel=info msg=\"  --k8s-require-ipv6-pod-cidr='false'\" subsys=daemon\r\nlevel=info msg=\"  --k8s-watcher-endpoint-selector='metadata.name!=kube-scheduler,metadata.name!=kube-controller-manager,metadata.name!=etcd-operator,metadata.name!=gcp-controller-manager'\" subsys=daemon\r\nlevel=info msg=\"  --k8s-watcher-"]
[22.75661, "o", "queue-size='1024'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hlevel=info msg=\"  --keep-bpf-templates='false'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hlevel=info msg=\"  --keep-config='false'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hlevel=info msg=\"  --kvstore='etcd'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[2S\u001b[48;1Hlevel=info msg=\"  --kvstore-opt='map[etcd.config:/var/lib/etcd-config/etcd.config]'\" subsys=daemon\r\nlevel=info msg=\"  --kvstore-periodic-sync='5m0s'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[3S\u001b[47;1Hlevel=info msg=\"  --label-prefix-file=''\" subsys=daemon\r\nlevel=info msg=\"  --labels=''\" subsys=daemon\r\nlevel=info msg=\"  --lb=''\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[2S\u001b[48;1Hlevel=info msg=\"  --lib-dir='/var/lib/cilium'\" subsys=daemon\r\nlevel=info msg=\"  --log-driver=''\" subsys=daemon\u001b[1;51r\u001b[50;1H"]
[23.005301, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hlevel=info msg=\"  --log-opt='map[]'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hlevel=info msg=\"  --log-system-load='false'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hlevel=info msg=\"  --masquerade='true'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hlevel=info msg=\"  --max-controller-interval='0'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[2S\u001b[48;1Hlevel=info msg=\"  --metrics=''\" subsys=daemon\r\nlevel=info msg=\"  --monitor-aggregation='None'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[5S\u001b[45;1Hlevel=info msg=\"  --monitor-queue-size='32768'\" subsys=daemon\r\nlevel=info msg=\"  --mtu='0'\" subsys=daemon\r\nlevel=info msg=\"  --nat46-range='0:0:0:0:0:FFFF::/96'\" subsys=daemon\r\nlevel=info msg=\"  --policy-queue-size='100'\" subsys=daemon\r\nlevel=info msg=\"  --pprof='false'\" subsys=daemon"]
[23.006381, "o", "\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[7S\u001b[43;1Hlevel=info msg=\"  --preallocate-bpf-maps='true'\" subsys=daemon\r\nlevel=info msg=\"  --prefilter-device='undefined'\" subsys=daemon\r\nlevel=info msg=\"  --prefilter-mode='native'\" subsys=daemon\r\nlevel=info msg=\"  --prepend-iptables-chains='true'\" subsys=daemon\r\nlevel=info msg=\"  --prometheus-serve-addr=''\" subsys=daemon\r\nlevel=info msg=\"  --proxy-connect-timeout='1'\" subsys=daemon\r\nlevel=info msg=\"  --restore='true'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[5S\u001b[45;1Hlevel=info msg=\"  --sidecar-http-proxy='false'\" subsys=daemon\r\nlevel=info msg=\"  --sidecar-istio-proxy-image='cilium/istio_proxy'\" subsys=daemon\r\nlevel=info msg=\"  --single-cluster-route='false'\" subsys=daemon\r\nlevel=info msg=\"  --socket-path='/var/run/cilium/cilium.sock'\" subsys=daemon\r\nlevel=info msg=\"  --sockops-enable='false'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[4S\u001b[46;1Hlevel=info msg=\"  --state-dir='/var/run/cilium'\" subsys=daemon\r\nlevel=info msg=\"  --tofqdns-dns-reject-response-code='refused'\" subsys=daemon\r\nlevel=info "]
[23.006521, "o", "msg=\"  --tofqdns-enable-poller='false'\" subsys=daemon\r\nlevel=info msg=\"  --tofqdns-enable-poller-events='true'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[5S\u001b[45;1Hlevel=info msg=\"  --tofqdns-endpoint-max-ip-per-hostname='50'\" subsys=daemon\r\nlevel=info msg=\"  --tofqdns-min-ttl='0'\" subsys=daemon\r\nlevel=info msg=\"  --tofqdns-pre-cache=''\" subsys=daemon\r\nlevel=info msg=\"  --tofqdns-proxy-port='0'\" subsys=daemon\r\nlevel=info msg=\"  --trace-payloadlen='128'\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[5S\u001b[45;1Hlevel=info msg=\"  --tunnel='vxlan'\" subsys=daemon\r\nlevel=info msg=\"  --version='false'\" subsys=daemon\r\nlevel=info msg=\"     _ _ _\" subsys=daemon\r\nlevel=info msg=\" ___|_| |_|_ _ _____\" subsys=daemon\r\nlevel=info msg=\"|  _| | | | | |     |\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hlevel=info msg=\"|___|_|_|_|___|_|_|_|\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[5S\u001b[45;1Hlevel=info msg=\"Cilium 1.5.3  go version go1.11.13 linux/amd64\" subsys=daemon\r\nlevel=info msg=\"Envoy version check disabled\" subsys=daemon\r\nlevel=info msg="]
[23.006988, "o", "\"clang (7.0.1) and kernel (4.12.14) versions: OK!\" subsys=daemon\r\nlevel=info msg=\"linking environment: OK!\" subsys=daemon\r\nlevel=info msg=\"bpf_requirements check: OK!\" subsys=daemon\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[9S\u001b[41;1Hlevel=info msg=\"bpf_features check: OK!\" subsys=daemon\r\nlevel=info msg=\"Detected mounted BPF filesystem at /sys/fs/bpf\" subsys=bpf\r\nlevel=info msg=\"Valid label prefix configuration:\" subsys=labels-filter\r\nlevel=info msg=\" - :io.kubernetes.pod.namespace\" subsys=labels-filter\r\nlevel=info msg=\" - :io.cilium.k8s.namespace.labels\" subsys=labels-filter\r\nlevel=info msg=\" - !:io.kubernetes\" subsys=labels-filter\r\nlevel=info msg=\" - !:.*kubernetes.io\" subsys=labels-filter\r\nlevel=info msg=\" - !:pod-template-generation\" subsys=labels-filter\r\nlevel=info msg=\" - !:pod-template-hash\" subsys=labels-filter\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hlevel=info msg=\" - !:controller-revision-hash\" subsys=labels-filter\u001b[1;51r\u001b[50;1H"]
[23.009819, "o", "\u001b[1;50r\u001b[50S\u001b[1;1Hlevel=info msg=\" - !:etcd_node\" subsys=labels-filter\r\nlevel=info msg=\"Initializing daemon\" subsys=daemon\r\nlevel=info msg=\"Detected MTU 1500\" subsys=mtu\r\nlevel=info msg=\"Restore service IDs from BPF maps\" duration=5.032478ms failed=0 removed=0 restored=0 skipped=0 subsys=daemon\r\nlevel=info msg=\"Removing stale endpoint interfaces\" subsys=daemon\r\nlevel=info msg=\"Establishing connection to apiserver\" host=\"https://10.96.0.1:443\" subsys=k8s\r\nlevel=info msg=\"Connected to apiserver\" subsys=k8s\r\nlevel=info msg=\"Retrieved node information from kubernetes\" nodeName=my-worker-3 subsys=k8s\r\nlevel=info msg=\"Received own node information from API server\" ipAddr.ipv4=172.28.0.8 ipAddr.ipv6=\"<nil>\" nodeName=my-worker-3 subsys=k8s v4Prefix=10.244.5.0/24 v6Prefix=\"f00d::af4:500:0:0/96\"\r\nlevel=info msg=\"k8s mode: Allowing localhost to reach local endpoints\" subsys=daemon\r\nlevel=info msg=\"Initializing node addressing\" subsys=daemon\r\nlevel=info msg=\"Initializing IPAM\" subsys=daemon v4Prefix=10.244.5.0/24 v6Prefi"]
[23.010246, "o", "x=\"f00d::af4:500:0:0/112\"\r\nlevel=info msg=\"Container runtime options set: endpoint=/var/run/containerd/containerd.sock,endpoint=/var/run/crio/crio.sock,datapath-mode=veth,endpoint=unix:///var/run/docker.sock\" subsys=daemon\r\nlevel=info msg=\"Restoring endpoints...\" subsys=daemon\r\nlevel=info msg=\"No old endpoints found.\" subsys=daemon\r\nlevel=info msg=\"Addressing information:\" subsys=daemon\r\nlevel=info msg=\"  Cluster-Name: default\" subsys=daemon\r\nlevel=info msg=\"  Cluster-ID: 0\" subsys=daemon\r\nlevel=info msg=\"  Local node-name: my-worker-3\" subsys=daemon\r\nlevel=info msg=\"  Node-IPv6: fc00::10ca:1\" subsys=daemon\r\nlevel=info msg=\"  IPv6 node prefix: f00d::af4:500:0:0/96\" subsys=daemon\r\nlevel=info msg=\"  IPv6 allocation prefix: f00d::af4:500:0:0/112\" subsys=daemon\r\nlevel=info msg=\"  IPv6 router address: f00d::af4:500:0:1\" subsys=daemon\r\nlevel=info msg=\"  External-Node IPv4: 172.28.0.8\" subsys=daemon\r\nlevel=info msg=\"  Internal-Node IPv4: 10.244.5.1\" subsys=daemon\r\nlevel=info msg=\"  Cluster IPv4 prefix: 10.0.0.0/8\" s"]
[23.010346, "o", "ubsys=daemon\r\nlevel=info msg=\"  IPv4 allocation prefix: 10.244.5.0/24\" subsys=daemon\r\nlevel=info msg=\"  Loopback IPv4: 10.244.5.83\" subsys=daemon\r\nlevel=info msg=\"Annotating k8s node\" subsys=daemon v4CiliumHostIP.IPv4=10.244.5.1 v4Prefix=10.244.5.0/24 v4healthIP.IPv4=10.244.5.231 v6CiliumHostIP.IPv6=\"f00d::af4:500:0:1\" v6Prefix=\"f00d::af4:500:0:0/96\" v6healthIP.IPv6=\"f00d::af4:500:0:ba0b\"\r\nlevel=info msg=\"Adding local node to cluster\" subsys=nodediscovery\r\nlevel=info msg=\"Sockmap disabled.\" subsys=sockops\r\nlevel=info msg=\"Sockmsg Disabled.\" subsys=sockops\r\nlevel=info msg=\"Initializing identity allocator\" subsys=identity-cache\r\nlevel=info msg=\"Setting sysctl net.core.bpf_jit_enable=1\" subsys=daemon\r\nlevel=info msg=\"Setting sysctl net.ipv4.conf.all.rp_filter=0\" subsys=daemon\r\nlevel=info msg=\"Setting sysctl net.ipv6.conf.all.disable_ipv6=0\" subsys=daemon\r\nlevel=info msg=\"Validating configured node address ranges\" subsys=daemon\r\nlevel=info msg=\"Starting connection tracking garbage collector\" subsys=daemon\r\nlevel="]
[23.010429, "o", "info msg=\"Initial scan of connection tracking completed\" subsys=endpoint-manager\r\nlevel=info msg=\"Launching node monitor daemon\" subsys=daemon\r\nlevel=info msg=\"Enabling k8s event listener\" subsys=daemon\r\nlevel=info msg=\"Starting IP identity watcher\" subsys=ipcache\r\nlevel=info msg=\"Envoy: Starting xDS gRPC server listening on /var/run/cilium/xds.sock\" subsys=envoy-manager\r\nlevel=info msg=\"Serving cilium node monitor v1.0 API at unix:///var/run/cilium/monitor.sock\" subsys=cilium-node-monitor\r\nlevel=info msg=\"Serving cilium node monitor v1.2 API at unix:///var/run/cilium/monitor1_2.sock\" subsys=cilium-node-monitor\r\nlevel=info msg=\"Beginning to read cilium agent events\" subsys=cilium-node-monitor\r\nlevel=info msg=\"CRD (CustomResourceDefinition) is installed and up-to-date\" name=CiliumNetworkPolicy/v2 subsys=k8s\r\nlevel=info msg=\"Updating CRD (CustomResourceDefinition)...\" name=v2.CiliumEndpoint subsys=k8s\r\nlevel=info msg=\"CRD (CustomResourceDefinition) is installed and up-to-\u001b[1;51r\u001b[50;71H\u001b[1;50r\u001b[34S\u001b[16;71Hdate"]
[23.011992, "o", "\" name=v2.CiliumEndpoint subsys=k8s\r\nlevel=info msg=\"Connecting to etcd server...\" config=/var/lib/etcd-config/etcd.config endpoints=\"[https://172.28.0.15:2379 https://172.28.0.31:2379]\" subsys=kvstore\r\nlevel=info msg=\"Waiting until all pre-existing resources related to policy have been received\" subsys=daemon\r\nlevel=info msg=\"Regenerating 0 restored endpoints\" subsys=daemon\r\nlevel=info msg=\"Enabling CRI event listener\" subsys=workload-watcher\r\nlevel=info msg=\"Launching Cilium health daemon\" subsys=daemon\r\nlevel=info msg=\"Launching Cilium health endpoint\" subsys=daemon\r\nlevel=info msg=\"All pre-existing resources related to policy have been received; continuing\" subsys=daemon\r\nlevel=info msg=\"Finished regenerating restored endpoints\" regenerated=0 subsys=daemon total=0\r\nlevel=info msg=\"Starting to watch allocation changes\" kvstoreErr=\"<nil>\" kvstoreStatus=\"No connection to etcd\" prefix=cilium/state/identities/v1/id subsys=allocator\r\nlevel=info msg=\"Spawning health endpoint with arguments []string{\\\"cilium-heal"]
[23.012345, "o", "th\\\", \\\"cilium_health\\\", \\\"cilium\\\", \\\"f00d::af4:500:0:ba0b/128\\\", \\\"10.244.5.231/32\\\", \\\"cilium-health\\\", \\\"-d --admin=unix --passive --pidfile /var/run/cilium/state/health-endpoint.pid\\\"}\" subsys=cilium-health-launcher\r\nlevel=info msg=\"Resolving identity labels (blocking)\" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 identityLabels=\"reserved:health\" ipv4=10.244.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ subsys=endpoint\r\nlevel=info msg=\"Identity of endpoint changed\" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=0 identity=4 identityLabels=\"reserved:health\" ipv4=10.244.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ oldIdentity=\"no identity\" subsys=endpoint\r\nlevel=info msg=\"regenerating all endpoints due to one or more identities created or deleted\" subsys=endpoint-manager\r\nlevel=info msg=\"Successfully verified version of etcd endpoint\" config=/var/lib/etcd-config/etcd.config endpoints=\"[https://172.28.0.15:2379 https://172.28.0.31:2379]\" etcdEndpoin"]
[23.012635, "o", "t=\"https://172.28.0.15:2379\" subsys=kvstore version=3.3.11\r\nlevel=info msg=\"Successfully verified version of etcd endpoint\" config=/var/lib/etcd-config/etcd.config endpoints=\"[https://172.28.0.15:2379 https://172.28.0.31:2379]\" etcdEndpoint=\"https://172.28.0.31:2379\" subsys=kvstore version=3.3.11\r\nlevel=info msg=\"Initializing Cilium API\" subsys=daemon\r\nlevel=info msg=\"Serving cilium health at http://[::]:4240\" subsys=health-server\r\nlevel=info msg=\"Daemon initialization completed\" bootstrapTime=34.167223016s subsys=daemon\r\nlevel=info msg=\"Serving cilium at unix:///var/run/cilium/cilium.sock\" subsys=daemon\r\nlevel=info msg=\"New endpoint\" containerID= datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1222 identity=4 ipv4=10.244.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ subsys=endpoint\r\nlevel=info msg=\"Serving cilium health at http://[::]:4240\" subsys=health-server\r\nlevel=info msg=\"New endpoint\" containerID=256460290f datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1433 ipv4=10.244.5.132"]
[23.012736, "o", " ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc subsys=endpoint\r\nlevel=info msg=\"Resolving identity labels (blocking)\" containerID=256460290f datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1433 identityLabels=\"k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=kured,k8s:io.kubernetes.pod.namespace=kube-system,k8s:name=kured\" ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc subsys=endpoint\r\nlevel=info msg=\"Allocating key\" key=\"k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=kured,k8s:io.kubernetes.pod.namespace=kube-system,k8s:name=kured\" subsys=allocator\r\nlevel=info msg=\"Reusing existing global key\" key=\"k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=kured;k8s:io.kubernetes.pod.namespace=kube-system;k8s:name=kured;\" subsys=allocator\r\nlevel=info msg=\"Identity of endpoint changed\" containerID=256460290f datapathPolicyRevision=0 desiredPolicyRevision=0"]
[23.261545, "o", "\u001b[1;51r\u001b[50;118H\u001b[1;50r\u001b[8S\u001b[42;118H endpointID=1433 identity=47423 identityLabels=\"k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=kured,k8s:io.kubernetes.pod.namespace=kube-system,k8s:name=kured\" ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc oldIdentity=\"no identity\" subsys=endpoint\r\nlevel=info msg=\"Waiting for endpoint to be generated\" containerID=256460290f datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1433 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc subsys=daemon\r\nlevel=info msg=\"Compiled new BPF template\" BPFCompilationTime=5.367513224s file-path=/var/run/cilium/state/templates/8cb32420663cbe6f23ed00c6bc7c51b59c20b78c/bpf_lxc.o subsys=datapath-loader\r\nlevel=info msg=\"Rewrote endpoint BPF program\" containerID= datapathPolicyRevision=0 desiredPolicyRevision=5 endpointID=1222 error=\"<nil>\" identity=4 ipv4=10.244.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ subsys=endpoint\r\nlevel=info msg"]
[23.261924, "o", "=\"Completed endpoint regeneration\" bpfCompilation=5.367513224s bpfLoadProg=6.542328156s bpfWaitForELF=5.367864973s bpfWriteELF=\"583.538µ\u001b[62C\n\u001b[49;151Hs\" buildDuration=11.929471603s containerID= datapathPolicyRevision=5 desiredPolicyRevision=5 endpointID=1222 identity=4 ipv4=10.244.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ mapSync=1.876491ms policyCalculation=\"192.461µ\u001b[50C\n\u001b[49;163Hs\" prepareBuild=5.596289ms proxyConfiguration=\"20.44µs\" proxyPolicyCalculation=\"52.271µs\" proxyWaitForAck=\"2.759µs\" reason=\"health daemon bootstrap\" subsys=endpoint waitingForCTClean=\"66.987µs\" waitingForLock=\"3.761µ\u001b[5S\u001b[5As\"\r\nlevel=info msg=\"Rewrote endpoint BPF program\" containerID=256460290f datapathPolicyRevision=0 desiredPolicyRevision=5 endpointID=1433 error=\"<nil>\" identity=47423 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc subsys=endpoint\r\nlevel=info msg=\"Successful endpoint creation\" containerID=256460290f datapathPolicyRevision=5 desiredPolicyRevision=5 endpointID=1433 ide"]
[23.262006, "o", "ntity=47423 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc subsys=daemon\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=6.593019652s bpfWaitForELF=4.627536377s bpfWriteELF=\"637.372µ\u001b[72C\n\u001b[49;141Hs\" buildDuration=11.229331256s containerID=256460290f datapathPolicyRevision=5 desiredPolicyRevision=5 endpointID=1433 identity=47423 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc mapSync=1.063947ms policyCalculation=\"170.037µs\" prepareBuild=\"851.384µ\u001b[50;213H\n\rs\" proxyConfiguration=\"15.557µs\" proxyPolicyCalculation=\"46.156µs\" proxyWaitForAck=\"2.415µs\" reason=\"updated security labels\" subsys=endpoint waitingForCTClean=\"62.399µs\" waitingForLock=\"3.22µ\u001b[10S\u001b[10As\"\r\nlevel=info msg=\"Serving cilium health at unix:///var/run/cilium/health.sock\" subsys=health-server\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=6.103515625e-05 newInterval=7m30s subsys=map-ct\r\nlevel=error msg=k8sErro"]
[23.262867, "o", "r error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:524: Failed to watch *v1.Service: Get https://10.96.0.1:443/api/v1/services?resourceVersion=2735648&timeoutSeconds=543&watch=true: dial tcp 10.96.0.1:443: connect: connection refused\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:454: Failed to watch *v1.NetworkPolicy: Get https://10.96.0.1:443/apis/networking.k8s.io/v1/networkpolicies?resourceVersion=2735654&timeoutSeconds=410&watch=true: dial tcp 10.96.0.1:443: connect: connection refused\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:933: Failed to watch *v1.Node: Get https://10.96.0.1:443/api/v1/nodes?resourceVersion=3337621&timeoutSeconds=345&watch=true: dial tcp 10.96.0.1:443: connect: connection refused\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:755: Failed to watch *v2.CiliumNetworkPolicy: Get https://10.96.0.1:443/apis/cilium.io/v2/ciliumnetworkpolicies?resou"]
[23.262981, "o", "rceVersion=2735662&timeoutSeconds=324&watch=true: dial tcp 10.96.0.1:443: connect: connection refused\" subsys\u001b[1;51r\u001b[50;93H"]
[23.26849, "o", "\u001b[1;50r\u001b[32S\u001b[18;93H=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:454: Failed to list *v1.NetworkPolicy: Get https://10.96.0.1:443/apis/networking.k8s.io/v1/networkpolicies?limit=500&resourceVersion=0: dial tcp 10.96.0.1:443: connect: connection refused\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:933: Failed to list *v1.Node: Get https://10.96.0.1:443/api/v1/nodes?limit=500&resourceVersion=0: dial tcp 10.96.0.1:443: connect: connection refused\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:524: Failed to list *v1.Service: Get https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0: dial tcp 10.96.0.1:443: connect: connection refused\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:755: Failed to list *v2.CiliumNetworkPolicy: Get https://10.96.0.1:443/apis/cilium.io/v2/ciliumnetworkpolicies?limit=500&resourceVersion=0: dial tcp "]
[23.26859, "o", "10.96.0.1:443: connect: connection refused\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:828: Failed to watch *v1.Pod: Get https://10.96.0.1:443/api/v1/pods?resourceVersion=3335941&timeoutSeconds=538&watch=true: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:594: Failed to watch *v1.Endpoints: Get https://10.96.0.1:443/api/v1/endpoints?fieldSelector=metadata.name%21%3Detcd-operator%2Cmetadata.name%21%3Dgcp-controller-manager%2Cmetadata.name%21%3Dkube-controller-manager%2Cmetadata.name%21%3Dkube-scheduler&resourceVersion=3335792&timeoutSeconds=411&watch=true: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:988: Failed to watch *v1.Namespace: Get https://10.96.0.1:443/api/v1/namespaces?resourceVersion=2735648&timeoutSeconds=398&watch=true: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\""]
[23.26865, "o", "github.com/cilium/cilium/daemon/k8s_watcher.go:828: Failed to list *v1.Pod: Get https://10.96.0.1:443/api/v1/pods?limit=500&resourceVersion=0: dial tcp 10.96.0.1:443: connect: connection refused\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:524: Failed to list *v1.Service: Get https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:755: Failed to list *v2.CiliumNetworkPolicy: Get https://10.96.0.1:443/apis/cilium.io/v2/ciliumnetworkpolicies?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:454: Failed to list *v1.NetworkPolicy: Get https://10.96.0.1:443/apis/networking.k8s.io/v1/networkpolicies?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/"]
[23.268708, "o", "k8s_watcher.go:933: Failed to list *v1.Node: Get https://10.96.0.1:443/api/v1/nodes?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:988: Failed to list *v1.Namespace: Get https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:594: Failed to list *v1.Endpoints: Get https://10.96.0.1:443/api/v1/endpoints?fieldSelector=metadata.name%21%3Detcd-operator%2Cmetadata.name%21%3Dgcp-controller-manager%2Cmetadata.name%21%3Dkube-controller-manager%2Cmetadata.name%21%3Dkube-scheduler&limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:828: Failed to list *v1.Pod: Get https://10.96.0.1:443/api/v1/pods?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=erro"]
[23.786589, "o", "r msg=k8sError error\u001b[1;51r\u001b[50;31H\u001b[1;50r\u001b[25S\u001b[25;31H=\"github.com/cilium/cilium/daemon/k8s_watcher.go:454: Failed to list *v1.NetworkPolicy: Get https://10.96.0.1:443/apis/networking.k8s.io/v1/networkpolicies?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:933: Failed to list *v1.Node: Get https://10.96.0.1:443/api/v1/nodes?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:755: Failed to list *v2.CiliumNetworkPolicy: Get https://10.96.0.1:443/apis/cilium.io/v2/ciliumnetworkpolicies?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:524: Failed to list *v1.Service: Get https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=info msg=\"Conntrack"]
[23.786856, "o", " garbage collector interval recalculated\" deleteRatio=0.00014495849609375 newInterval=11m15s subsys=map-ct\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:594: Failed to list *v1.Endpoints: Get https://10.96.0.1:443/api/v1/endpoints?fieldSelector=metadata.name%21%3Detcd-operator%2Cmetadata.name%21%3Dgcp-controller-manager%2Cmetadata.name%21%3Dkube-controller-manager%2Cmetadata.name%21%3Dkube-scheduler&limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:988: Failed to list *v1.Namespace: Get https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:828: Failed to list *v1.Pod: Get https://10.96.0.1:443/api/v1/pods?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daem"]
[23.78695, "o", "on/k8s_watcher.go:988: Failed to list *v1.Namespace: namespaces is forbidden: User \\\"system:serviceaccount:kube-system:cilium\\\" cannot list resource \\\"namespaces\\\" in API group \\\"\\\" at the cluster scope\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:454: Failed to list *v1.NetworkPolicy: Get https://10.96.0.1:443/apis/networking.k8s.io/v1/networkpolicies?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:755: Failed to list *v2.CiliumNetworkPolicy: Get https://10.96.0.1:443/apis/cilium.io/v2/ciliumnetworkpolicies?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=error msg=k8sError error=\"github.com/cilium/cilium/daemon/k8s_watcher.go:524: Failed to list *v1.Service: Get https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0: net/http: TLS handshake timeout\" subsys=k8s\r\nlevel=info msg=\"regenerating all endpoints due to Kubern"]
[23.787026, "o", "etes service endpoint updated\" subsys=endpoint-manager\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=4.933162ms containerID=256460290f datapathPolicyRevision=6 desiredPolicyRevision=6 endpointID=1433 identity=47423 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc mapSync=2.031922ms policyCalculation=\"316.273µs\" prepareBuild=1.280688ms proxyConfiguration=\"22.455µ\u001b[2C\n\u001b[49;211Hs\" proxyPolicyCalculation=\"531.519µs\" proxyWaitForAck=\"3.3µs\" reason=\"Kubernetes service endpoint updated\" subsys=endpoint waitingForCTClean=984ns waitingForLock=\"4.392µ\u001b[2S\u001b[2As\"\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=4.997842ms containerID= datapathPolicyRevision=6 desiredPolicyRevision=6 endpointID=1222 identity=4 ipv4=10.244.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ mapSync=1.809695ms policyCalculation=\"291.451µs\" prepareB"]
[23.787084, "o", "uild=\"735.135µs\" proxyConfiguration=\"16.786µs\" proxyPolicyCalculation=1.57"]
[23.787337, "o", "\u001b[1;51r\u001b[50;206H\u001b[1;50r\u001b[50;213H\n\u001b[49;206H1377ms proxyWaitForAck=\"2.223µs\" reason=\"Kubernetes service endpoint updated\" subsys=endpoint waitingForCTClean=623ns waitingForLock=\"3.478µ\u001b[3S\u001b[3As\"\r\nlevel=info msg=\"regenerating all endpoints due to Kubernetes service endpoint updated\" subsys=endpoint-manager\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=13.480454ms containerID=256460290f datapathPolicyRevision=7 desiredPolicyRevision=7 endpointID=1433 identity=47423 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc mapSync=10.610306ms policyCalculation=\"255.192µs\" prepareBuild=\"966.183µs\" proxyConfiguration=\"16.917µ\u001b[50;213H\n\rs\" proxyPolicyCalculation=\"123.769µs\" proxyWaitForAck=\"2.73µs\" reason=\"Kubernetes service endpoint updated\" subsys=endpoint waitingForCTClean=959ns waitingForLock=\"3.774µ\u001b[2S\u001b[2As\"\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitFo"]
[23.787562, "o", "rELF=0s bpfWriteELF=0s buildDuration=7.726549ms containerID= datapathPolicyRevision=7 desiredPolicyRevision=7 endpointID=1222 identity=4 ipv4=10.244.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ mapSync=3.042573ms policyCalculation=\"350.956µs\" prepareBuild=2.588659ms proxyConfiguration=\"23.073µs\" proxyPolicyCalculation=\"274.317µ\u001b[3C\n\u001b[49;210Hs\" proxyWaitForAck=\"3.451µs\" reason=\"Kubernetes service endpoint updated\" subsys=endpoint waitingForCTClean=\"1.005µs\" waitingForLock=\"4.137µ\u001b[5S\u001b[5As\"\r\nlevel=warning msg=\"Unable to update ipcache entry of Kubernetes node\" error=\"ipcache entry owned by kvstore or agent\" subsys=daemon\r\nlevel=warning msg=\"Unable to update ipcache entry of Kubernetes node\" error=\"ipcache entry owned by kvstore or agent\" subsys=daemon\r\nlevel=info msg=\"regenerating all endpoints due to Kubernetes service endpoint updated\" subsys=endpoint-manager\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=4.986908ms cont"]
[23.787604, "o", "ainerID=256460290f datapathPolicyRevision=8 desiredPolicyRevision=8 endpointID=1433 identity=47423 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc mapSync=1.958261ms policyCalculation=\"384.333µs\" prepareBuild=1.512737ms proxyConfiguration=\"22.242µ\u001b[2C\n\u001b[49;211Hs\" proxyPolicyCalculation=\"330.866µs\" proxyWaitForAck=\"3.347µs\" reason=\"Kubernetes service endpoint updated\" subsys=endpoint waitingForCTClean=965ns waitingForLock=\"4.502µ\u001b[2S\u001b[2As\"\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=4.704724ms containerID= datapathPolicyRevision=8 desiredPolicyRevision=8 endpointID=1222 identity=4 ipv4=10.244.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ mapSync=2.048402ms policyCalculation=\"372.165µs\" prepareBuild=1.227894ms proxyConfiguration=\"24.248µs\" proxyPolicyCalculation=\"300.432µ\u001b[3C\n\u001b[49;210Hs\" proxyWaitForAck=\"3.107µs\" reason=\"Kubernetes service endpoint updated\" subsys=endpoint waitingForCT"]
[23.787626, "o", "Clean=931ns waitingForLock=\"4.096µ\u001b[3S\u001b[3As\"\r\nlevel=info msg=\"regenerating all endpoints due to Kubernetes service endpoint updated\" subsys=endpoint-manager\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=5.118556ms containerID=256460290f datapathPolicyRevision=9 desiredPolicyRevision=9 endpointID=1433 identity=47423 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc mapSync=1.98278ms policyCalculation=\"264.153µs\" prepareBuild=1.89165ms proxyConfiguration=\"22.329µ\u001b[4C\n\u001b[49;209Hs\" proxyPolicyCalculation=\"170.392µs\" proxyWaitForAck=\"3.327µs\" reason=\"Kubernetes service endpoint updated\" subsys=endpoint waitingForCTClean=\"1.055µs\" waitingForLock=\"4.676µ\u001b[2S\u001b[2As\"\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=7.187176ms containerID= datapathPolicyRevision=9 desiredPolicyRevision=9 endpointID=1222 identity=4 ipv4=10.2"]
[23.787648, "o", "44.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ mapSync=3.280031ms policyCalculation=\"265.397µs\" prepareBuild=2.548199ms proxyConfiguration=\"15.891µs\" p\u001b[1;51r\u001b[50;179HroxyPolicyCalculation=\"159.978µ\u001b[1;50r\u001b[50;213H\n\u001b[49;210Hs\" proxyWaitForAck=\"2.434µs\" reason=\"Kubernetes service endpoint updated\" subsys=endpoint waitingForCTClean=627ns waitingForLock=\"3.813µ\u001b[31S\u001b[19ds\"\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=0.00019073486328125 newInterval=16m53s subsys=map-ct\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=0.000316619873046875 newInterval=25m20s subsys=map-ct\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=0.000484466552734375 newInterval=38m0s subsys=map-"]
[23.787674, "o", "ct\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=0.00072479248046875 newInterval=57m0s subsys=map-ct\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=0.001087188720703125 newInterval=1h25m30s subsys=map-ct\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=0.0016326904296875 newInterval=2h8m15s subsys=map-ct\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=0.002437591552734375 newInterval=3h12m23s subsys=map-ct\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=no"]
[23.787843, "o", "de-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=0.003673553466796875 newInterval=4h48m35s subsys=map-ct\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=info msg=\"C"]
[23.787948, "o", "onntrack garbage collector interval recalculated\" deleteRatio=0.005489349365234375 newInterval=7h12m53s subsys=map-ct\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=info msg=\"Conntrack garbage collector interval recalculated\" deleteRatio=0.008243560791015625 newInterval=10h49m20s subsys=map-ct\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received no"]
[23.787982, "o", "de delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=info msg=\"Conntrack garbage collector interval recalculat"]
[24.07492, "o", "ed\" deleteRatio=0.01232147216796875 newInterval=12h0\u001b[1;51r\u001b[50;116H\u001b[1;50r\u001b[34S\u001b[16;116Hm0s subsys=map-ct\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node "]
[24.075069, "o", "mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared wit"]
[24.075548, "o", "hin 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warnin"]
[24.075923, "o", "g msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for no"]
[24.076213, "o", "de mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg\u001b[1;51r\u001b[50;18H\u001b[1;50r\u001b[33S\u001b[17;18H=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mr"]
[24.076458, "o", "ostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared withi"]
[24.076669, "o", "n 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning "]
[24.076866, "o", "msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Received node delete event for node mrostecki-master-3 which re-appeared within 30s\" subsys=node-store\r\nlevel=warning msg=\"Unable to update ipcache entry of K"]
[24.07706, "o", "ubernetes node\" error=\"ipcache entry owned by kvstore or agent\" subsys=daemon\r\nlevel=warning msg=\"Unable to update ipcache\u001b[1;51r\u001b[50;44H\u001b[1;50r\u001b[13S\u001b[37;44H entry of Kubernetes node\" error=\"ipcache entry owned by kvstore or agent\" subsys=daemon\r\nlevel=info msg=\"New endpoint\" containerID=bfb25149d5 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3924 ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs subsys=endpoint\r\nlevel=info msg=\"Resolving identity labels (blocking)\" containerID=bfb25149d5 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3924 identityLabels=\"k8s:class=deathstar,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=default,k8s:io.kubernetes.pod.namespace=default,k8s:org=empire\" ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs subsys=endpoint\r\nlevel=info msg=\"Allocating key\" key=\"k8s:class=deathstar,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy"]
[24.077247, "o", ".serviceaccount=default,k8s:io.kubernetes.pod.namespace=default,k8s:org=empire\" subsys=allocator\r\nlevel=info msg=\"regenerating all endpoints due to one or more identities created or deleted\" subsys=endpoint-manager\r\nlevel=info msg=\"Invalid state transition skipped\" containerID=bfb25149d5 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3924 endpointState.from=waiting-for-identity endpointState.to=waiting-to-regenerate file=/home/abuild/rpmbuild/BUILD/go/src/github.com/cilium/cilium/pkg/endpointma"]
[24.077584, "o", "nager/manager.go ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs line=427 subsys=endpoint\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=4.002281ms containerID=256460290f datapathPolicyRevision=11 desiredPolicyRevision=11 endpointID=1433 identity=47423 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc mapSync=1.616651ms policyCalculation=\"251.025µs\" prepareBuild=1.233056ms proxyConfiguration=\"12.673µ\n\u001b[As\" proxyPolicyCalculation=\"260.473µs\" proxyWaitForAck=\"2.081µs\" reason=\"one or more identities created or deleted\" subsys=endpoint waitingForCTClean=599ns waitingForLock=\"2.574µ\u001b[2S\u001b[2As\"\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=27.782492ms containerID= datapathPolicyRevision=11 desiredPolicyRevision=11 endpointID=1222 identity=4 ipv4=10.244.5.231 ipv6=\"f00d::af4:500"]
[24.077758, "o", ":0:ba0b\" k8sPodName=/ mapSync=1.242991ms policyCalculation=\"223.33µs\" prepareBuild=3.222779ms proxyConfiguration=\"14.597µs\" proxyPolicyCalculation=\"189.095µ\u001b[C\n\u001b[49;212Hs\" proxyWaitForAck=\"2.081µs\" reason=\"one or more identities created or deleted\" subsys=endpoint waitingForCTClean=603ns waitingForLock=\"2.73µ\u001b[10S\u001b[10As\"\r\nlevel=info msg=\"Allocated new global key\" key=\"k8s:class=deathstar;k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=default;k8s:io.kubernetes.pod.namespace=default;k8s:org=empire;\" subsys=allocator\r\nlevel=info msg=\"Identity of endpoint changed\" containerID=bfb25149d5 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3924 identity=41745 identityLabels=\"k8s:class=deathstar,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=default,k8s:io.kubernetes.pod.namespace=default,k8s:org=empire\" ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs oldIdentity=\"no identity\" subsys=endpoint\r\nl"]
[24.077848, "o", "evel=info msg=\"Waiting for endpoint to be generated\" containerID=bfb25149d5 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=3924 ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs subsys=daemon\r\nlevel=info msg=\"New endpoint\" containerID=df2ddae127 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1260 ipv4=10.244.5.233 ipv6=\"f00d::af4:500:0:3dee\" k8sPodName=default/xwing subsys=endpoint\r\nlevel=info msg=\"Resolving identity labels (blocking)\" containerID=df2ddae127 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1260 identityLabels=\"k8s:class=xwing,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceacco\u001b[1;51r\u001b[50;39H\u001b[1;50r\u001b[13S\u001b[37;39Hunt=default,k8s:io.kubernetes.pod.namespace=default,k8s:org=alliance\" ipv4=10.244.5.233 ipv6=\"f00d::af4:500:0:3dee\" k8sPodName=default/xwing subsys=endpoint\r\nlevel=info msg=\"Allocating key\" key=\"k8s:class=xwing,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceac"]
[24.078005, "o", "count=default,k8s:io.kubernetes.pod.namespace=default,k8s:org=alliance\" subsys=allocator\r\nlevel=info msg=\"Allocated new global key\" key=\"k8s:class=xwing;k8s:io.cilium.k8s.policy.cluster=default;k8s:io.cilium.k8s.policy.serviceaccount=default;k8s:io.kubernetes.pod.namespace=default;k8s:org=alliance;\" subsys=allocator\r\nlevel=info msg=\"Identity of endpoint changed\" containerID=df2ddae127 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1260 identity=59864 identityLabels=\"k8s:class=xwing,k8s:io.cilium.k8s.policy.cluster=default,k8s:io.cilium.k8s.policy.serviceaccount=default,k8s:io.kubernetes.pod.namespace=default,k8s:org=alliance\" ipv4=10.244.5.233 ipv6=\"f00d::af4:500:0:3dee\" k8sPodName=default/xwing oldIdentity=\"no identity\" subsys=endpoint\r\nlevel=info msg=\"Waiting for endpoint to be generated\" containerID=df2ddae127 datapathPolicyRevision=0 desiredPolicyRevision=0 endpointID=1260 ipv4=10.244.5.233 ipv6=\"f00d::af4:500:0:3dee\" k8sPodName=default/xwing subsys=daemon\r\nlevel=info msg=\"regenerating all en"]
[24.078094, "o", "dpoints due to one or more identities created or deleted\" subsys=endpoint-manager\r\nlevel=info msg=\"Rewrote endpoint BPF program\" containerID=bfb25149d5 datapathPolicyRevision=0 desiredPolicyRevision=11 endpointID=3924 error=\"<nil>\" identity=41745 ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs subsys=endpoint\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=4.899522779s bpfWaitForELF=\"7.877µs\" bpfWriteELF=\"644.62µ\u001b[76C\n\u001b[49;137Hs\" buildDuration=4.981043302s containerID=bfb25149d5 datapathPolicyRevision=11 desiredPolicyRevision=11 endpointID=3924 identity=41745 ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs mapSync=1.089711ms policyCalculation=\"157.72µ\u001b[19C\n\u001b[49;194Hs\" prepareBuild=\"720.83µs\" proxyConfiguration=\"13.88µs\" proxyPolicyCalculation=\"40.678µs\" proxyWaitForAck=\"2.706µs\" reason=\"updated security labels\" subsys=endpoint waitingForCTClean=70.814ms waitingForLock=\"239.48µ\u001b[4S\u001b[4As\"\r"]
[24.078565, "o", "\nlevel=info msg=\"Successful endpoint creation\" containerID=bfb25149d5 datapathPolicyRevision=11 desiredPolicyRevision=11 endpointID=3924 identity=41745 ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs subsys=daemon\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=5.476687ms containerID= datapathPolicyRevision=12 desiredPolicyRevision=12 endpointID=1222 identity=4 ipv4=10.244.5.231 ipv6=\"f00d::af4:500:0:ba0b\" k8sPodName=/ mapSync=2.264238ms policyCalculation=\"200.557µs\" prepareBuild=2.301295ms proxyConfiguration=\"21.824µs\" proxyPolicyCalculation=\"117.348µ\u001b[C\n\u001b[49;212Hs\" proxyWaitForAck=\"2.441µs\" reason=\"one or more identities created or deleted\" subsys=endpoint waitingForCTClean=737ns waitingForLock=\"3.901µ\u001b[2S\u001b[2As\"\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=5.585923ms containerID=256460290f datapathP"]
[24.07878, "o", "olicyRevision=12 desiredPolicyRevision=12 endpointID=1433 identity=47423 ipv4=10.244.5.132 ipv6=\"f00d::af4:500:0:8d57\" k8sPodName=kube-system/kured-w8vpc mapSync=1.474898ms policyCalculation=\"171.639µs\" prepareBuild=3.276768ms proxyConfiguration=\"16.488µ\n\u001b[As\" proxyPolicyCalculation=\"113.631µs\" proxyWaitForAck=\"2.552µs\" reason=\"one or more identities created or deleted\" subsys=endpoint waitingForCTClean=681ns waitingForLock=\"3.199µ\u001b[2S\u001b[2As\"\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilat"]
[24.297226, "o", "ion=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=14.5621ms containerID=bfb25149d5 datapathPolicyRevision=12 desiredPolicyRevision=12 endpointID=3924 identity=41745 ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs mapSync=1.500221ms poli\u001b[1;51r\u001b[50;144HcyCalculation=\"162.868µs\" prepareBuild=\"843.567µ\u001b[1;50r\u001b[50;213H\n\u001b[49;192Hs\" proxyConfiguration=\"15.8µs\" proxyPolicyCalculation=\"119.511µs\" proxyWaitForAck=\"2.3µs\" reason=\"one or more identities created or deleted\" subsys=endpoint waitingForCTClean=587ns waitingForLock=\"3.475µ\u001b[3S\u001b[3As\"\r\nlevel=info msg=\"Rewrote endpoint BPF program\" containerID=df2ddae127 datapathPolicyRevision=0 desiredPolicyRevision=12 endpointID=1260 error=\"<nil>\" identity=59864 ipv4=10.244.5.233 ipv6=\"f00d::af4:500:0:3dee\" k8sPodName=default/xwing subsys=endpoint\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=6.523200566s bpfWaitForELF=\"9.416µs\" bpfWriteELF=\"716.448µ\u001b[75C\n\u001b[49;138Hs\" buildDur"]
[24.297423, "o", "ation=6.673073979s containerID=df2ddae127 datapathPolicyRevision=12 desiredPolicyRevision=12 endpointID=1260 identity=59864 ipv4=10.244.5.233 ipv6=\"f00d::af4:500:0:3dee\" k8sPodName=default/xwing mapSync=1.109551ms policyCalculation=\"166.09µs\" prepareBuild=\"685.287µ\u001b[12C\n\u001b[49;201Hs\" proxyConfiguration=\"14.248µs\" proxyPolicyCalculation=\"41.889µs\" proxyWaitForAck=\"2.618µs\" reason=\"updated security labels\" subsys=endpoint waitingForCTClean=137.967764ms waitingForLock=\"3.128µ\u001b[4S\u001b[4As\"\r\nlevel=info msg=\"Successful endpoint creation\" containerID=df2ddae127 datapathPolicyRevision=12 desiredPolicyRevision=12 endpointID=1260 identity=59864 ipv4=10.244.5.233 ipv6=\"f00d::af4:500:0:3dee\" k8sPodName=default/xwing subsys=daemon\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=11.481039ms containerID=df2ddae127 datapathPolicyRevision=12 desiredPolicyRevision=12 endpointID=1260 identity=59864 ipv4=10.244.5.233 ipv6=\"f00d::af4:500:0:3dee\" k8s"]
[24.297516, "o", "PodName=default/xwing mapSync=1.93539ms policyCalculation=\"359.392µs\" prepareBuild=\"889.135µs\" proxyConfiguration=\"14.367µ\u001b[9C\n\u001b[49;204Hs\" proxyPolicyCalculation=7.720872ms proxyWaitForAck=\"1.953µs\" reason=\"one or more identities created or deleted\" subsys=endpoint waitingForCTClean=617ns waitingForLock=\"3.782µ\u001b[8S\u001b[8As\"\r\nlevel=info msg=\"Policy Add Request\" ciliumNetworkPolicy=\"[&{EndpointSelector:{\\\"matchLabels\\\":{\\\"any:class\\\":\\\"deathstar\\\",\\\"any:org\\\":\\\"empire\\\",\\\"k8s:io.kubernetes.pod.namespace\\\":\\\"default\\\"}} Ingress:[{FromEndpoints:[{\\\"matchLabels\\\":{\\\"any:org\\\":\\\"empire\\\",\\\"k8s:io.kubernetes.pod.namespace\\\":\\\"default\\\"}}] FromRequires:[] ToPorts:[{Ports:[{Port:80 Protocol:TCP}] Rules:<nil>}] FromCIDR:[] FromCIDRSet:[] FromEntities:[] aggregatedSelectors:[{LabelSelector:0xc0009d53a0 requirements:0xc0009d54a0}]}] Egress:[] Labels:[k8s:io.cilium.k8s.policy.name=rule1 k8s:io.cilium.k8s.policy.uid=e280517d-64f3-4eeb-bcee-dd96d982a1b0 k8s:io.cilium.k8s.policy.namespace=default k8s:io.cilium.k8s.policy.d"]
[24.297601, "o", "erived-from=CiliumNetworkPolicy] Description:}]\" policyAddRequest=eac18491-f001-11e9-987e-fa163e7c69a9 subsys=daemon\r\nlevel=info msg=\"Policy imported via API, recalculating...\" policyAddRequest=eac18491-f001-11e9-987e-fa163e7c69a9 policyRevision=13 subsys=daemon\r\nlevel=info msg=\"Imported CiliumNetworkPolicy\" ciliumNetworkPolicyName=rule1 k8sApiVersion= k8sNamespace=default subsys=daemon\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=4.577662ms containerID=bfb25149d5 datapathPolicyRevision=13 desiredPolicyRevision=13 endpointID=3924 identity=41745 ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs mapSync=1.581546ms policyCalculation=\"309.014µs\" prepareBuild=\"977.95µ\u001b[21C\n\u001b[49;192Hs\" proxyConfiguration=\"16.688µs\" proxyPolicyCalculation=\"110.522µs\" proxyWaitForAck=\"2.427µs\" reason=\"policy rules added\" subsys=endpoint waitingForCTClean=645ns waitingForLock=\"3.142µ\u001b[3S\u001b[3As\"\r\nlevel=info m"]
[24.297701, "o", "sg=\"Policy Add Request\" ciliumNetworkPolicy=\"[&{EndpointSelector:{\\\"matchLabels\\\":{\\\"any:class\\\":\\\"deathstar\\\",\\\"any:org\\\":\\\"empire\\\",\\\"k8s:io.kubernetes.pod.namespace\\\":\\\"default\\\"}} Ingress:[{FromEndpoints:[{\\\"matchLabels\\\":{\\\"any:org\\\":\\\"empire\\\",\\\"k8s:io.kubernetes.pod.namespace\\\":\\\"default\\\"}}] FromRequires:[] ToPorts:[{Ports:[{Port:80 Protocol:TCP}] Rules:0xc0008caee0}] FromCIDR:[] FromCIDRSet:[] FromEntities:[] aggreg\u001b[1;51r\u001b[50;15H\u001b[1;50r\u001b[14S\u001b[36;15HatedSelectors:[{LabelSelector:0xc0011c5680 requirements:0xc0011c57e0}]}] Egress:[] Labels:[k8s:io.cilium.k8s.policy.name=rule1 k8s:io.cilium.k8s.policy.uid=e280517d-64f3-4eeb-bcee-dd96d982a1b0 k8s:io.cilium.k8s.policy.namespace=default k8s:io.cilium.k8s.policy.derived-from=CiliumNetworkPolicy] Description:}]\" policyAddRequest=442187a0-f002-11e9-987e-fa163e7c69a9 subsys=daemon\r\nlevel=info msg=\"Policy imported via API, recalculating...\" policyAddRequest=442187a0-f002-11e9-987e-fa163e7c69a9 policyRevision=15 subsys=daemon\r\nlevel=info msg=\"Imported CiliumNetw"]
[24.297799, "o", "orkPolicy\" ciliumNetworkPolicyName=rule1 k8sApiVersion= k8sNamespace=default subsys=daemon\r\nlevel=warning msg=\"Envoy: Failed to start proxy\" error=\"exec: \\\"cilium-envoy\\\": executable file not found in $PATH\" subsys=envoy-manager\r\nlevel=warning msg=\"Unable to create http proxy, will retry\" error=\"3924:ingress:TCP:80: Envoy proxy process failed to start, cannot add redirect\" id=\"3924:ingress:TCP:80\" subsys=proxy\r\nlevel=warning msg=\"Unable to create http proxy, will retry\" error=\"3924:ingress:TCP:80: Envoy proxy process failed to start, cannot add redirect\" id=\"3924:ingress:TCP:80\" subsys=proxy\r\nlevel=warning msg=\"Unable to create http proxy, will retry\" error=\"3924:ingress:TCP:80: Envoy proxy process failed to start, cannot add redirect\" id=\"3924:ingress:TCP:80\" subsys=proxy\r\nlevel=warning msg=\"Unable to create http proxy, will retry\" error=\"3924:ingress:TCP:80: Envoy proxy process failed to start, cannot add redirect\" id=\"3924:ingress:TCP:80\" subsys=proxy\r\nlevel=warning msg=\"Unable to create http proxy, will r"]
[24.298311, "o", "etry\" error=\"3924:ingress:TCP:80: Envoy proxy process failed to start, cannot add redirect\" id=\"3924:ingress:TCP:80\" subsys=proxy\r\nlevel=error msg=\"Unable to create http proxy\" error=\"3924:ingress:TCP:80: Envoy proxy process failed to start, cannot add redirect\" id=\"3924:ingress:TCP:80\" subsys=proxy\r\nlevel=warning msg=\"generating BPF for endpoint failed, keeping stale directory.\" containerID=bfb25149d5 datapathPolicyRevision=13 desiredPolicyRevision=15 endpointID=3924 file-path=3924_next_fail identity=41745 ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs subsys=endpoint\r\nlevel=info msg=\"Completed endpoint regeneration\" bpfCompilation=0s bpfLoadProg=0s bpfWaitForELF=0s bpfWriteELF=0s buildDuration=76.247185ms containerID=bfb25149d5 datapathPolicyRevision=13 desiredPolicyRevision=15 endpointID=3924 identity=41745 ipv4=10.244.5.128 ipv6=\"f00d::af4:500:0:bf9a\" k8sPodName=default/deathstar-d7d9cc8b-d2vxs mapSync=\"512.203µs\" policyCalculation=\"319.363µs\" prepareBuild=\"58."]
[24.298627, "o", "113µ\u001b[19C\n\u001b[49;194Hs\" proxyConfiguration=74.817723ms proxyPolicyCalculation=\"259.453µs\" proxyWaitForAck=0s reason=\"policy rules added\" subsys=endpoint waitingForCTClean=0s waitingForLock=\"1.894µ\u001b[5S\u001b[5As\"\r\nlevel=info msg=\"Beginning to read perf buffer\" startTime=\"2019-10-16 11:05:42.566412925 +0000 UTC m=+2108830.826617195\" subsys=cilium-node-monitor\r\nlevel=info msg=\"Stopped reading perf buffer\" startTime=\"2019-10-16 11:05:42.566412925 +0000 UTC m=+2108830.826617195\" subsys=cilium-node-monitor\r\nlevel=info msg=\"Beginning to read perf buffer\" startTime=\"2019-10-16 11:06:18.163971961 +0000 UTC m=+2108866.424176127\" subsys=cilium-node-monitor\r\nlevel=info msg=\"Stopped reading perf buffer\" startTime=\"2019-10-16 11:06:18.163971961 +0000 UTC m=+2108866.424176127\" subsys=cilium-node-monitor\u001b[1;51r\u001b[50;1H"]
[26.121796, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[26.404562, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[26.643481, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[26.863504, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[27.074468, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[27.279244, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[27.509594, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[27.719226, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[28.534941, "o", "^C"]
[28.56523, "o", "\u001b[1;50r\u001b[50;213H\n\rsles@caasp-master-mrostecki-caasp-cluster-0:~> \u001b[1;51r\u001b[50;48H"]
[31.415643, "o", "\u001b[?25l\r\n\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:10 16-Oct-19\u001b(B\u001b[m\u001b[50;48H\u001b[?12l\u001b[?25h\r\u001b[25P(reverse-i-search)`':\u001b[C"]
[31.916272, "o", "\u001b[3De': kubectl -n kube-system logs -f cilium-2ztnx\u001b[23D"]
[32.154298, "o", "\u001b[21Gx': kubectl -n kube-system exec -ti cilium-2ztnx -- cilium monitor --type drop\u001b[48G"]
[32.375521, "o", "\u001b[22G\u001b[1@e\u001b[26C"]
[32.695631, "o", "\u001b[23G\u001b[1@c\u001b[26C"]
[33.58658, "o", "\r\u001b[21@sles@caasp-master-mrostecki-caasp-cluster-0:~>\u001b[24C"]
[33.587162, "o", "\u001b[C"]
[34.94619, "o", "\u001b[3C"]
[35.135375, "o", "\u001b[4C"]
[35.272689, "o", "\u001b[7C"]
[35.437499, "o", "\u001b[6C"]
[35.604517, "o", "\u001b[10C"]
[36.062462, "o", "\u001b[8C"]
[36.635271, "o", "\u001b[C\u001b[7Pdrop\u001b[5D"]
[36.8521, "o", "\u001b[K"]
[37.773616, "o", "\b\u001b[K"]
[37.941244, "o", "\b\u001b[K"]
[38.077312, "o", "\b\u001b[K"]
[38.245412, "o", "\b\u001b[K"]
[38.386536, "o", "\b\u001b[K"]
[38.564971, "o", "\b\u001b[K"]
[38.699668, "o", "\b\u001b[K"]
[39.379206, "o", "s"]
[39.559542, "o", "t"]
[39.690225, "o", "a"]
[39.80954, "o", "t"]
[39.940266, "o", "u"]
[40.116581, "o", "s"]
[40.512382, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[41.772042, "o", "KVStore:        \u001b[?25l\r\n\u001b[30m\u001b[42m[0] 0:kubectl*                                                                                                                                                                \"caasp-master-mrosteck\" 11:10 16-Oct-19\u001b(B\u001b[m\u001b[50;17H\u001b[?12l\u001b[?25h"]
[41.773336, "o", "\u001b[1;50r\u001b[5S\u001b[45;17H        Ok   etcd: 2/2 connected, has-quorum=true: https://172.28.0.15:2379 - 3.3.11 (Leader); https://172.28.0.31:2379 - 3.3.11\r\nContainerRuntime:       Ok   cri-o client: Ok - cri daemon: Ok\r\nKubernetes:             Ok   1.15 (v1.15.2) [linux/amd64]\r\nKubernetes APIs:        [\"CustomResourceDefinition\", \"cilium/v2::CiliumNetworkPolicy\", \"core/v1::Endpoint\", \"core/v1::Namespace\", \"core/v1::Node\", \"core/v1::Pods\", \"core/v1::Service\", \"networking.k8s.io/v1::NetworkPolicy\"]\r\nCilium:                 \u001b[1;51r\u001b[50;25H"]
[41.774823, "o", "\u001b[1;50r\u001b[3S\u001b[47;25HOk   OK\r\nNodeMonitor:            Listening for events on 2 CPUs with 64x4096 of shared memory\r\nCilium health daemon:   Ok   \r\nIPv4 address pool:      \u001b[1;51r\u001b[50;25H\u001b[1;50r\u001b[3S\u001b[47;25H6/255 allocated from 10.244.5.0/24\r\nIPv6 address pool:      6/65535 allocated from f00d::af4:500:0:0/112\r\nController Status:      30/31 healthy"]
[41.775659, "o", "\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1H  Name                                        Last success   Last error   Count   Message\r\n  sync-cnp-policy-status (v2 default/rule1)   26m26s ago     39s ago      31      context deadline exceeded   \u001b[1;51r\u001b[50;111H"]
[41.77605, "o", "\u001b[1;50r\u001b[2S\u001b[49;1HProxy Status:   OK, ip 10.244.5.1, port-range 10000-20000\u001b[1;51r\u001b[50;1H"]
[41.778907, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;1HCluster health:   6/6 reachable   (2019-10-16T11:08:52Z)\u001b[1;51r\u001b[50;1H"]
[41.792618, "o", "sles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[43.538761, "o", "\u001b[?25l\r\n\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:10 16-Oct-19\u001b(B\u001b[m\u001b[50;48H\u001b[?12l\u001b[?25h"]
[43.539075, "o", "kubectl -n kube-system exec -ti cilium-2ztnx -- cilium status"]
[44.565786, "o", "\u001b[6D"]
[45.253157, "o", "\b"]
[45.726287, "o", "- status\u001b[7D"]
[46.053773, "o", "h status\u001b[7D"]
[46.188965, "o", "e status\u001b[7D"]
[46.318759, "o", "a status\u001b[7D"]
[46.434538, "o", "l status\u001b[7D"]
[46.60038, "o", "t status\u001b[7D"]
[46.682643, "o", "h status\u001b[7D"]
[53.894169, "o", "\u001b[?25l\r\n\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:11 16-Oct-19\u001b(B\u001b[m\u001b[50;109H\u001b[?12l\u001b[?25h"]
[54.327187, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[54.329608, "o", "\u001b[?25l\r\n\u001b[30m\u001b[42m[0] 0:kubectl*                                                                                                                                                                \"caasp-master-mrosteck\" 11:11 16-Oct-19\u001b(B\u001b[m\u001b[50;1H\u001b[?12l\u001b[?25h"]
[55.389821, "o", "\u001b[1;50r\u001b[2S\u001b[48;1HProbe time:   2019-10-16T11:08:52Z\r\nNodes:\r\n  my-worker-3 (localhost):\u001b[1;51r\u001b[50;27H"]
[55.393934, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[55.398304, "o", "    Host connectivity to 172.28.0.8:"]
[55.398848, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H      ICMP to stack:   "]
[55.3998, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;24HOK, RTT=1.542144ms\r\n      HTTP to agent:   OK, RTT=649.174µ\u001b[2S\u001b[2As\r\n    Endpoint connectivity to 10.244.5.231:\r\n      ICMP to stack:\u001b[1;51r\u001b[50;21H"]
[55.400376, "o", "   OK, RTT=1.43241ms"]
[55.401353, "o", "\u001b[1;50r\u001b[50;213H\n\r      HTTP to agent:   \u001b[1;51r\u001b[50;24H\u001b[1;50r\u001b[2S\u001b[48;24HOK, RTT=1.242312ms\r\n  mrostecki-master-1:\u001b[1;51r\u001b[50;1H    Host connectivity to 172.28.0.31:"]
[55.641033, "o", "\u001b[1;50r\u001b[50;213H\n\r      ICMP to stack:\u001b[1;51r\u001b[50;21H\u001b[1;50r\u001b[50;213H\n\u001b[49;21H   OK, RTT=4.814106ms\r\n      HTTP to agent:\u001b[1;51r\u001b[50;21H   OK, RTT=2.332081ms\u001b[1;50r\u001b[50;213H\n\r    Endpoint connectivity to 10.244.0.92:\u001b[1;51r\u001b[50;42H\u001b[1;50r\u001b[3S\u001b[48;1H      ICMP to stack:   OK, RTT=4.856385ms\r\n      HTTP to agent:   OK, RTT=1.24152ms\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[2S\u001b[48;1H  mrostecki-master-2:\r\n    Host connectivity to 172.28.0.15:\u001b[1;51r\u001b[50;1H      ICMP to stack:   OK, RTT=4.085644ms\u001b[1;50r\u001b[2S\u001b[49;1H      HTTP to agent:   OK, RTT=10.26699ms\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1H    Endpoint connectivity to 10.244.1.98:\r\n      ICMP to stack:   \u001b[1;51r\u001b[50;24H\u001b[1;50r\u001b[50;213H\n\u001b[49;24HOK, RTT=4.787224ms\r\n      HTTP to agent:\u001b[1;51r\u001b[50;21H\u001b[1;50r\u001b[3S\u001b[47;21H   OK, RTT=20.789998ms\r\n  mrostecki-master-3:\r\n    Host connectivity to 172.28.0.18:\u001b[1;51r\u001b[50;1H"]
[55.641487, "o", "\u001b[1;50r\u001b[4S\u001b[46;1H      ICMP to stack:   OK, RTT=2.385285ms\r\n      HTTP to agent:   OK, RTT=3.60194ms\r\n    Endpoint connectivity to 10.244.2.177:\r\n      ICMP to stack:   OK, RTT=2.72495ms\r\n      HTTP to agent:   OK, RTT=2.297123ms\u001b[1;51r\u001b[50;42H\u001b[1;50r\u001b[7S\u001b[44;1H  my-worker-1:\r\n    Host connectivity to 172.28.0.21:\r\n      ICMP to stack:   OK, RTT=4.166878ms\r\n      HTTP to agent:   OK, RTT=4.3634ms\r\n    Endpoint connectivity to 10.244.3.187:\r\n      ICMP to stack:   OK, RTT=4.581355ms\r\n      HTTP to agent:   \u001b[1;51r\u001b[50;24H\u001b[1;50r\u001b[7S\u001b[43;24HOK, RTT=7.07796ms\r\n  my-worker-2:\r\n    Host connectivity to 172.28.0.12:\r\n      ICMP to stack:   OK, RTT=4.058422ms\r\n      HTTP to agent:   OK, RTT=2.660285ms\r\n    Endpoint connectivity to 10.244.4.142:\r\n      ICMP to stack:   OK, RTT=4.249197ms\u001b[1;51r\u001b[50;1H      HTTP to agent:   OK, RTT=3.051529ms\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1Hsles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[64.113462, "o", "\u001b[?25l\r\n\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:11 16-Oct-19\u001b(B\u001b[m\u001b[50;48H\u001b[?12l\u001b[?25hkubectl -n kube-system exec -ti cilium-2ztnx -- cilium-health status"]
[65.750749, "o", "\u001b[6D"]
[66.341203, "o", "\u001b[7D"]
[66.909804, "o", "\u001b[6P status\u001b[7D"]
[67.530479, "o", "\u001b[K"]
[68.331465, "o", "\b\u001b[K"]
[68.684429, "o", " "]
[68.836234, "o", "m"]
[69.025603, "o", "o"]
[69.261815, "o", "n"]
[69.404262, "o", "i"]
[69.509052, "o", "t"]
[69.645519, "o", "o"]
[69.753214, "o", "r"]
[70.005974, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[71.253219, "o", "\u001b[1;50r\u001b[2S\u001b[48;1HListening for events on 2 CPUs with 64x4096 of shared memory\r\nPress Ctrl-C to quit\u001b[?25l\u001b[1;51r\u001b[51;1H\u001b[30m\u001b[42m[0] 0:kubectl*                                                                                                                                                                \"caasp-master-mrosteck\" 11:11 16-Oct-19\u001b(B\u001b[m\u001b[50;1H\u001b[?12l\u001b[?25h"]
[72.052578, "o", "\u001b[1;50r\u001b[9S\u001b[41;1Hlevel=info msg=\"Initializing dissection cache...\" subsys=monitor\r\n<- overlay flow 0x6a2e8215 identity 0->0 state new ifindex cilium_vxlan: fc00::10ca:1 -> f00d::af4:500:0:ba0b EchoRequest\r\n-> endpoint 1222 flow 0x6a2e8215 identity 1->4 state new ifindex cilium_health: fc00::10ca:1 -> f00d::af4:500:0:ba0b EchoRequest\r\n<- overlay flow 0x250f8e89 identity 0->0 state new ifindex cilium_vxlan: 10.244.3.1 -> 10.244.5.231 EchoRequest\r\n-> endpoint 1222 flow 0x250f8e89 identity 1->4 state new ifindex cilium_health: 10.244.3.1 -> 10.244.5.231 EchoRequest\r\n<- endpoint 1222 flow 0x5869ba99 identity 4->0 state new ifindex 0: f00d::af4:500:0:ba0b -> fc00::10ca:1 EchoReply\r\n-> host from flow 0x5869ba99 identity 4->1 state reply ifindex cilium_net: f00d::af4:500:0:ba0b -> fc00::10ca:1 EchoReply\r\n<- endpoint 1222 flow 0x250f8e89 identity 4->0 state new ifindex 0: 10.244.5.231 -> 10.244.3.1 EchoReply\r\n-> overlay flow 0x250f8e89 identity 4->0 state new ifindex cilium_vxlan: 10.244.5.231 -> 10.244.3.1 EchoReply"]
[72.052891, "o", "\u001b[1;51r\u001b[50;1H"]
[72.564681, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;1H<- overlay flow 0xc245959a identity 0->0 state new ifindex cilium_vxlan: 10.244.1.1 -> 10.244.5.231 EchoRequest\u001b[1;51r\u001b[50;1H"]
[72.565292, "o", "\u001b[1;50r\u001b[7S\u001b[43;1H-> endpoint 1222 flow 0xc245959a identity 1->4 state new ifindex cilium_health: 10.244.1.1 -> 10.244.5.231 EchoRequest\r\n<- endpoint 1222 flow 0xc245959a identity 4->0 state new ifindex 0: 10.244.5.231 -> 10.244.1.1 EchoReply\r\n-> overlay flow 0xc245959a identity 4->0 state new ifindex cilium_vxlan: 10.244.5.231 -> 10.244.1.1 EchoReply\r\n<- overlay flow 0x1e4a6624 identity 0->0 state new ifindex cilium_vxlan: fc00::10ca:1 -> f00d::af4:500:0:ba0b EchoRequest\r\n-> endpoint 1222 flow 0x1e4a6624 identity 1->4 state new ifindex cilium_health: fc00::10ca:1 -> f00d::af4:500:0:ba0b EchoRequest\r\n<- endpoint 1222 flow 0x5869ba99 identity 4->0 state new ifindex 0: f00d::af4:500:0:ba0b -> fc00::10ca:1 EchoReply\r\n-> host from flow 0x5869ba99 identity 4->1 state reply ifindex cilium_net: f00d::af4:500:0:ba0b -> fc00::10ca:1 EchoReply\u001b[1;51r\u001b[50;1H"]
[73.473172, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;1H<- host flow 0xb29822d0 identity 2->0 state new ifindex 0: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\u001b[1;51r\u001b[50;1H"]
[73.477361, "o", "\u001b[1;50r\u001b[7S\u001b[43;1H-> endpoint 1222 flow 0xb29822d0 identity 1->4 state new ifindex cilium_health: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n<- endpoint 1222 flow 0xec64aac5 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp SYN, ACK\r\n-> host from flow 0xec64aac5 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp SYN, ACK\r\n<- host flow 0xb29822d0 identity 2->0 state new ifindex 0: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp ACK\r\n-> endpoint 1222 flow 0xb29822d0 identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp ACK\r\n<- host flow 0xb29822d0 identity 2->0 state new ifindex 0: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp ACK\r\n-> endpoint 1222 flow 0xb29822d0 identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp ACK\u001b[1;51r\u001b[50;1H"]
[73.478292, "o", "\u001b[1;50r\u001b[5S\u001b[45;1H<- endpoint 1222 flow 0xec64aac5 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp ACK\r\n-> host from flow 0xec64aac5 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp ACK\r\n<- endpoint 1222 flow 0xec64aac5 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp ACK\r\n-> host from flow 0xec64aac5 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp ACK\r\n<- host flow 0xb29822d0 identity 2->0 state new ifindex 0: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp ACK\u001b[1;51r\u001b[50;1H"]
[73.482575, "o", "\u001b[1;50r\u001b[3S\u001b[47;1H-> endpoint 1222 flow 0xb29822d0 identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp ACK\r\n<- endpoint 1222 flow 0xec64aac5 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp ACK, FIN\r\n-> host from flow 0xec64aac5 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp ACK, FIN\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[4S\u001b[46;1H<- host flow 0xb29822d0 identity 2->0 state new ifindex 0: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp ACK, FIN\r\n-> endpoint 1222 flow 0xb29822d0 identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:34092 -> [f00d::af4:500:0:ba0b]:4240 tcp ACK, FIN\r\n<- endpoint 1222 flow 0xec64aac5 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp ACK\r\n-> host from flow 0xec64aac5 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:34092 tcp ACK\u001b[1;51r\u001b[5"]
[73.482716, "o", "0;1H"]
[74.578284, "o", "\u001b[1;50r\u001b[6S\u001b[44;1H<- host flow 0xc1582c62 identity 1->0 state new ifindex 0: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp SYN\r\n-> overlay flow 0xc1582c62 identity 1->0 state new ifindex cilium_vxlan: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp SYN\r\n<- overlay flow 0x3926f33e identity 0->0 state new ifindex cilium_vxlan: 10.244.1.98:4240 -> 10.244.5.1:52536 tcp SYN, ACK\r\n<- host flow 0xc1582c62 identity 1->0 state new ifindex 0: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp ACK\r\n-> overlay flow 0xc1582c62 identity 1->0 state new ifindex cilium_vxlan: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp ACK\r\n<- host flow 0xc1582c62 identity 1->0 state new ifindex 0: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp ACK\u001b[1;51r\u001b[50;1H"]
[74.580524, "o", "\u001b[1;50r\u001b[2S\u001b[48;1H-> overlay flow 0xc1582c62 identity 1->0 state new ifindex cilium_vxlan: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp ACK\r\n<- overlay flow 0x3926f33e identity 0->0 state new ifindex cilium_vxlan: 10.244.1.98:4240 -> 10.244.5.1:52536 tcp ACK\u001b[1;51r\u001b[50;1H"]
[74.582219, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;1H<- overlay flow 0x3926f33e identity 0->0 state new ifindex cilium_vxlan: 10.244.1.98:4240 -> 10.244.5.1:52536 tcp ACK\u001b[1;51r\u001b[50;1H"]
[74.584617, "o", "\u001b[1;50r\u001b[8S\u001b[42;1H<- overlay flow 0x3926f33e identity 0->0 state new ifindex cilium_vxlan: 10.244.1.98:4240 -> 10.244.5.1:52536 tcp ACK, FIN\r\n<- host flow 0xc1582c62 identity 1->0 state new ifindex 0: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp ACK\r\n-> overlay flow 0xc1582c62 identity 1->0 state new ifindex cilium_vxlan: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp ACK\r\n<- host flow 0xc1582c62 identity 1->0 state new ifindex 0: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp ACK, FIN\r\n-> overlay flow 0xc1582c62 identity 1->0 state new ifindex cilium_vxlan: 10.244.5.1:52536 -> 10.244.1.98:4240 tcp ACK, FIN\r\n<- overlay flow 0x3926f33e identity 0->0 state new ifindex cilium_vxlan: 10.244.1.98:4240 -> 10.244.5.1:52536 tcp ACK\r\n<- host flow 0x5633182e identity 2->0 state new ifindex 0: [fc00::10ca:1]:39224 -> [f00d::af4:100:0:f667]:4240 tcp SYN\r\n-> overlay flow 0x5633182e identity 1->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:39224 -> [f00d::af4:100:0:f667]:4240 tcp SYN\u001b[1;51r\u001b[50;1H"]
[74.618753, "o", "\u001b[1;50r\u001b[4S\u001b[46;1H<- overlay flow 0x8bd40a2d identity 0->0 state new ifindex cilium_vxlan: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp SYN\r\n-> endpoint 1222 flow 0x8bd40a2d identity 1->4 state new ifindex cilium_health: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp SYN\r\n<- endpoint 1222 flow 0x68ee643b identity 4->0 state new ifindex 0: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp SYN, ACK\r\n-> overlay flow 0x68ee643b identity 4->0 state new ifindex cilium_vxlan: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp SYN, ACK\u001b[1;51r\u001b[50;1H"]
[74.622699, "o", "\u001b[1;50r\u001b[9S\u001b[41;1H<- overlay flow 0x8bd40a2d identity 0->0 state new ifindex cilium_vxlan: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp ACK\r\n-> endpoint 1222 flow 0x8bd40a2d identity 1->4 state established ifindex cilium_health: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp ACK\r\n<- overlay flow 0x8bd40a2d identity 0->0 state new ifindex cilium_vxlan: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp ACK\r\n-> endpoint 1222 flow 0x8bd40a2d identity 1->4 state established ifindex cilium_health: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp ACK\r\n<- endpoint 1222 flow 0x68ee643b identity 4->0 state new ifindex 0: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp ACK\r\n-> overlay flow 0x68ee643b identity 4->0 state new ifindex cilium_vxlan: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp ACK\r\n<- endpoint 1222 flow 0x68ee643b identity 4->0 state new ifindex 0: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp ACK\r\n-> overlay flow 0x68ee643b identity 4->0 state new ifindex cilium_vxlan: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp ACK\r\n<- endpoint 1222 flow 0x68"]
[74.623137, "o", "ee643b identity 4->0 state new ifindex 0: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp ACK, FIN\u001b[1;51r\u001b[50;1H"]
[74.627753, "o", "\u001b[1;50r\u001b[13S\u001b[37;1H-> overlay flow 0x68ee643b identity 4->0 state new ifindex cilium_vxlan: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp ACK, FIN\r\n<- overlay flow 0x8bd40a2d identity 0->0 state new ifindex cilium_vxlan: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp ACK\r\n-> endpoint 1222 flow 0x8bd40a2d identity 1->4 state established ifindex cilium_health: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp ACK\r\n<- overlay flow 0x8bd40a2d identity 0->0 state new ifindex cilium_vxlan: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp ACK, FIN\r\n-> endpoint 1222 flow 0x8bd40a2d identity 1->4 state established ifindex cilium_health: 10.244.4.1:60018 -> 10.244.5.231:4240 tcp ACK, FIN\r\n<- endpoint 1222 flow 0x68ee643b identity 4->0 state new ifindex 0: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp ACK\r\n-> overlay flow 0x68ee643b identity 4->0 state new ifindex cilium_vxlan: 10.244.5.231:4240 -> 10.244.4.1:60018 tcp ACK\r\n<- overlay flow 0x96e79f22 identity 0->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 "]
[74.628243, "o", "tcp SYN\r\n-> endpoint 1222 flow 0x96e79f22 identity 1->4 state new ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n<- endpoint 1222 flow 0xb82ecce5 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK\r\n-> host from flow 0xb82ecce5 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK\r\n<- host flow 0x51512cea identity 2->0 state new ifindex 0: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\r\n-> endpoint 1222 flow 0x51512cea identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\u001b[1;51r\u001b[50;1H"]
[75.591541, "o", "\u001b[1;50r\u001b[2S\u001b[48;1H<- host flow 0x178fc1b6 identity 2->0 state new ifindex 0: [fc00::10ca:1]:39224 -> [f00d::af4:100:0:f667]:4240 tcp SYN\r\n-> overlay flow 0x178fc1b6 identity 1->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:39224 -> [f00d::af4:100:0:f667]:4240 tcp SYN\u001b[1;51r\u001b[50;1H"]
[75.629587, "o", "\u001b[1;50r\u001b[6S\u001b[44;1H<- overlay flow 0x2e3adc7f identity 0->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n-> endpoint 1222 flow 0x2e3adc7f identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n<- endpoint 1222 flow 0x340910e1 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK\r\n-> host from flow 0x340910e1 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK\r\n<- host flow 0x51512cea identity 2->0 state new ifindex 0: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\r\n-> endpoint 1222 flow 0x51512cea identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\u001b[1;51r\u001b[50;1H"]
[75.974894, "o", "\u001b[1;50r\u001b[2S\u001b[48;1H<- host flow 0xdfe2335f identity 2->0 state new ifindex 0: [fc00::10ca:1]:39208 -> [f00d::af4:100:0:f667]:4240 tcp SYN\r\n-> overlay flow 0xdfe2335f identity 1->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:39208 -> [f00d::af4:100:0:f667]:4240 tcp SYN\u001b[1;51r\u001b[50;1H"]
[77.612118, "o", "\u001b[1;50r\u001b[2S\u001b[48;1H<- host flow 0xd881d16f identity 2->0 state new ifindex 0: [fc00::10ca:1]:39224 -> [f00d::af4:100:0:f667]:4240 tcp SYN\r\n-> overlay flow 0xd881d16f identity 1->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:39224 -> [f00d::af4:100:0:f667]:4240 tcp SYN\u001b[1;51r\u001b[50;1H"]
[77.639365, "o", "\u001b[1;50r\u001b[6S\u001b[44;1H<- overlay flow 0xa35d5116 identity 0->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n-> endpoint 1222 flow 0xa35d5116 identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n<- endpoint 1222 flow 0xc0d51601 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK\r\n-> host from flow 0xc0d51601 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK\r\n<- host flow 0x51512cea identity 2->0 state new ifindex 0: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\r\n-> endpoint 1222 flow 0x51512cea identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\u001b[1;51r\u001b[50;1H"]
[78.795042, "o", "\u001b[1;50r\u001b[2S\u001b[48;1H<- host flow 0xe6bc1198 identity 2->0 state new ifindex 0: [fc00::10ca:1]:55048 -> [f00d::af4:0:0:c097]:4240 tcp SYN\r\n-> overlay flow 0xe6bc1198 identity 1->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:55048 -> [f00d::af4:0:0:c097]:4240 tcp SYN\u001b[1;51r\u001b[50;1H"]
[79.235944, "o", "\u001b[1;50r\u001b[2S\u001b[48;1H<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp SYN\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp SYN\u001b[1;51r\u001b[50;1H"]
[79.236623, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;1H<- endpoint 1433 flow 0x19083ff8 identity 47423->0 state new ifindex 0: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp SYN, ACK\u001b[1;51r\u001b[50;1H"]
[79.242017, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;1H-> host from flow 0x19083ff8 identity 47423->1 state reply ifindex cilium_net: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp SYN, ACK\u001b[1;51r\u001b[50;1H"]
[79.244466, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;1H<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\u001b[1;51r\u001b[50;1H"]
[79.245289, "o", "\u001b[1;50r\u001b[5S\u001b[45;1H-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\r\n<- endpoint 1433 flow 0x19083ff8 identity 47423->0 state new ifindex 0: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK\r\n-> host from flow 0x19083ff8 identity 47423->1 state reply ifindex cilium_net: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK\u001b[1;51r\u001b[50;1H"]
[79.256561, "o", "\u001b[1;50r\u001b[5S\u001b[45;1H<- endpoint 1433 flow 0x19083ff8 identity 47423->0 state new ifindex 0: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK\r\n-> host from flow 0x19083ff8 identity 47423->1 state reply ifindex cilium_net: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK, FIN\u001b[1;51r\u001b[50;1H"]
[79.257148, "o", "\u001b[1;50r\u001b[4S\u001b[46;1H-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK, FIN\r\n<- endpoint 1433 flow 0x19083ff8 identity 47423->0 state new ifindex 0: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK, FIN\r\n-> host from flow 0x19083ff8 identity 47423->1 state reply ifindex cilium_net: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK, FIN\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\u001b[1;51r\u001b[50;1H\u001b[1;50r\u001b[50;213H\n\u001b[49;1H-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\u001b[1;51r\u001b[50;1H"]
[80.114837, "o", "\u001b[1;50r\u001b[6S\u001b[44;1H<- overlay flow 0xf84fe30d identity 0->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n-> endpoint 1222 flow 0xf84fe30d identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n<- endpoint 1222 flow 0x510c3996 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:46252 tcp SYN, ACK\r\n-> host from flow 0x510c3996 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:46252 tcp SYN, ACK\r\n<- host flow 0x71da2157 identity 2->0 state new ifindex 0: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\r\n-> endpoint 1222 flow 0x71da2157 identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\u001b[1;51r\u001b[50;1H"]
[81.772269, "o", "\u001b[1;50r\u001b[6S\u001b[44;1H<- overlay flow 0xcb47353e identity 0->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n-> endpoint 1222 flow 0xcb47353e identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN\r\n<- endpoint 1222 flow 0xbab9da5e identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK\r\n-> host from flow 0xbab9da5e identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK\r\n<- host flow 0x51512cea identity 2->0 state new ifindex 0: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\r\n-> endpoint 1222 flow 0x51512cea identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST\u001b[1;51r\u001b[50;1H"]
[81.817706, "o", "\u001b[1;50r\u001b[3S\u001b[47;1H^C\r\nReceived an interrupt, disconnecting from monitor...\u001b[1;51r\u001b[50;1H"]
[81.845376, "o", "sles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[83.333635, "o", "\u001b[?25l\r\n\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:11 16-Oct-19\u001b(B\u001b[m\u001b[50;48H\u001b[?12l\u001b[?25h"]
[83.334351, "o", "kubectl -n kube-system exec -ti cilium-2ztnx -- cilium monitor"]
[84.498732, "o", " "]
[84.668569, "o", "-"]
[84.825315, "o", "-"]
[84.997303, "o", "t"]
[85.195772, "o", "y"]
[85.325466, "o", "p"]
[85.426744, "o", "e"]
[85.590503, "o", " "]
[85.775918, "o", "d"]
[85.976258, "o", "r"]
[86.048543, "o", "o"]
[86.281633, "o", "p"]
[86.564646, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[1;51r\u001b[50;1H"]
[87.890535, "o", "\u001b[1;50r\u001b[2S\u001b[48;1HListening for events on 2 CPUs with 64x4096 of shared memory\r\nPress Ctrl-C to quit\u001b[?25l\u001b[1;51r\u001b[51;1H\u001b[30m\u001b[42m[0] 0:kubectl*                                                                                                                                                                \"caasp-master-mrosteck\" 11:11 16-Oct-19\u001b(B\u001b[m\u001b[50;1H\u001b[?12l\u001b[?25h"]
[94.84318, "o", "\u001b[?25l\u001b[24A───────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[32m──────────────────────────────────────────────────────────────────────────────────────────────────────────\u001b[1;1H\u001b(B\u001b[m<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK, FIN                                                                                                       \u001b[K\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK,"]
[94.844146, "o", " FIN                                                                    \u001b[K\r\n<- endpoint 1433 flow 0x19083ff8 identity 47423->0 state new ifindex 0: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK, FIN                                                                                          \u001b[K\r\n-> host from flow 0x19083ff8 identity 47423->1 state reply ifindex cilium_net: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK, FIN                                                                                   \u001b[K\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\u001b[K\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK                                                                         \u001b[K\r\n<- overlay flow 0xf84fe30d identity 0->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN                                                       "]
[94.844395, "o", "                         \u001b[K\r\n-> endpoint 1222 flow 0xf84fe30d identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN                                                                 \u001b[K\r\n<- endpoint 1222 flow 0x510c3996 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:46252 tcp SYN, ACK                                                                                \u001b[K\r\n-> host from flow 0x510c3996 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:46252 tcp SYN, ACK                                                                         \u001b[K\r\n<- host flow 0x71da2157 identity 2->0 state new ifindex 0: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp RST                                                                                              \u001b[K\r\n-> endpoint 1222 flow 0x71da2157 identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:46252 -> [f00d::af4:500"]
[94.844453, "o", ":0:ba0b]:4240 tcp RST                                                                 \u001b[K\r\n<- overlay flow 0xcb47353e identity 0->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN                                                                                \u001b[K\r\n-> endpoint 1222 flow 0xcb47353e identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN                                                                 \u001b[K\r\n<- endpoint 1222 flow 0xbab9da5e identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK                                                                                \u001b[K\r\n-> host from flow 0xbab9da5e identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK                                                                         \u001b[K\r\n<- host flow 0x51512cea identity 2->0 state new ifindex 0: [fc00"]
[94.846587, "o", "::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST                                                                                              \u001b[K\r\n-> endpoint 1222 flow 0x51512cea identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST                                                                 \u001b[K\r\n^C\u001b[K\r\nReceived an interrupt, disconnecting from monitor...\u001b[K\r\n\u001b[K\r\nsles@caasp-master-mrostecki-caasp-cluster-0:~> kubectl -n kube-system exec -ti cilium-2ztnx -- cilium monitor --type drop                                                                                           \u001b[K\r\nListening for events on 2 CPUs with 64x4096 of shared memory\u001b[K\r\nPress Ctrl-C to quit\u001b[K\r\n\u001b[K\u001b[2B\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[K\r\n\u001b[30m\u001b[42m[0] 0:kubectl*                                                                                                                               "]
[94.846874, "o", "                                 \"caasp-master-mrosteck\" 11:11 16-Oct-19\u001b(B\u001b[m\u001b[27;1H\u001b[?12l\u001b[?25h\u001b[?25l\u001b[51d\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:11 16-Oct-19\u001b(B\u001b[m\u001b[27;1H\u001b[?12l\u001b[?25h"]
[95.012273, "o", "sles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[99.381478, "o", "\r\u001b[25P(reverse-i-search)`':\u001b[C"]
[112.824413, "o", "\u001b[3Dx': tmux\b"]
[113.048884, "o", "\u001b[6Dw': # It means that the access for xwing was restricted!\u001b[21D"]
[113.204413, "o", "\u001b[22G\u001b[1@i\u001b[34C"]
[113.41276, "o", "\u001b[23G\u001b[1@n\u001b[34C"]
[113.492771, "o", "\u001b[24G\u001b[1@g\u001b[34C"]
[113.908973, "o", "\u001b[?25l\u001b[51;1H\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:12 16-Oct-19\u001b(B\u001b[m\u001b[27;59H\u001b[?12l\u001b[?25h"]
[114.636165, "o", "\u001b[28Gkubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\u001b[41G"]
[116.826365, "o", "\r\u001b[20@sles@caasp-master-mrostecki-caasp-cluster-0:~>\r\n"]
[118.918468, "o", "\u001b[1;25r\u001b[2S\u001b[23;1Hlevel=info msg=\"Initializing dissection cache...\" subsys=monitor\r\nxx drop (Policy denied (L3)) flow 0x7c193def to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[28;1H"]
[119.946273, "o", "\u001b[1;25r\u001b[25;213H\n\u001b[24;1Hxx drop (Policy denied (L3)) flow 0x976518c1 to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[28;1H"]
[121.998353, "o", "\u001b[1;25r\u001b[25;213H\n\u001b[24;1Hxx drop (Policy denied (L3)) flow 0xc2b18fa2 to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[28;1H"]
[126.152131, "o", "\u001b[1;25r\u001b[25;213H\n\u001b[24;1Hxx drop (Policy denied (L3)) flow 0x47a4020f to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[28;1H"]
[133.998022, "o", "^C\r\nsles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[134.350533, "o", "\u001b[1;25r\u001b[25;213H\n\u001b[24;1Hxx drop (Policy denied (L3)) flow 0x36172a92 to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[29;48H"]
[135.723008, "o", "kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing"]
[136.651581, "o", "\u001b[48G\u001b[K"]
[137.266857, "o", "\r\u001b[25P(reverse-i-search)`':\u001b[C"]
[137.565381, "o", "\u001b[3Dt': kubectl exec xwing -- curl -s -XPOST deathstar.default.svc.cluster.local/v1/request-landing\u001b[9D"]
[137.65838, "o", "\u001b[21G\u001b[16Pi': kubectl -n kube-system exec -ti cilium-2ztnx -- cilium monitor --type drop\u001b[45D"]
[138.046806, "o", "\u001b[16D\u001b[1@f\u001b[16C"]
[138.164075, "o", "\u001b[16D\u001b[1@i\u001b[16C"]
[140.836054, "o", "\r\u001b[20@sles@caasp-master-mrostecki-caasp-cluster-0:~>\r\n\u001b[?25l\u001b[51d\u001b[30m\u001b[42m[0] 0:kubectl*                                                                                                                                                                \"caasp-master-mrosteck\" 11:12 16-Oct-19\u001b(B\u001b[m\u001b[30;1H\u001b[?12l\u001b[?25h"]
[142.86249, "o", "\u001b[1;25r\u001b[25;213H\n\u001b[24;1Hxx drop (Policy denied (L3)) flow 0x6b6e6479 to endpoint 3924, identity 30182->41745: 10.244.4.108:58872 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[30;1H"]
[143.898856, "o", "\u001b[1;25r\u001b[25;213H\n\u001b[24;1Hxx drop (Policy denied (L3)) flow 0x6b6e6479 to endpoint 3924, identity 30182->41745: 10.244.4.108:58872 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[30;1H"]
[145.896169, "o", "\u001b[1;25r\u001b[25;213H\n\u001b[24;1Hxx drop (Policy denied (L3)) flow 0x6b6e6479 to endpoint 3924, identity 30182->41745: 10.244.4.108:58872 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[30;1H"]
[150.121613, "o", "\u001b[1;25r\u001b[25;213H\n\u001b[24;1Hxx drop (Policy denied (L3)) flow 0x6b6e6479 to endpoint 3924, identity 30182->41745: 10.244.4.108:58872 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[30;1H"]
[150.472019, "o", "\u001b[1;25r\u001b[25;213H\n\u001b[24;1Hxx drop (Policy denied (L3)) flow 0xf8e93a9 to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN\u001b[1;51r\u001b[30;1H"]
[154.792166, "o", "^C"]
[154.801028, "o", "\r\nsles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[155.387675, "o", "\u001b[?25l\u001b[51;1H\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:12 16-Oct-19\u001b(B\u001b[m\u001b[31;48H\u001b[?12l\u001b[?25h"]
[155.388019, "o", "logout\r\n"]
[155.639109, "o", "\u001b[?25l\u001b[H<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp SYN\u001b[K\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp SYN                                                                         \u001b[K\r\n<- endpoint 1433 flow 0x19083ff8 identity 47423->0 state new ifindex 0: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp SYN, ACK                                                                                          \u001b[K\r\n-> host from flow 0x19083ff8 identity 47423->1 state reply ifindex cilium_net: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp SYN, ACK                                                                                   \u001b[K\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\u001b[K\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK       "]
[155.639223, "o", "                                                                  \u001b[K\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\u001b[K\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK                                                                         \u001b[K\r\n<- endpoint 1433 flow 0x19083ff8 identity 47423->0 state new ifindex 0: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK                                                                                               \u001b[K\r\n-> host from flow 0x19083ff8 identity 47423->1 state reply ifindex cilium_net: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK                                                                                        \u001b[K\r\n<- endpoint 1433 flow 0x19083ff8 identity 47423->0 state new ifindex 0: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK                                                                            "]
[155.639674, "o", "                   \u001b[K\r\n-> host from flow 0x19083ff8 identity 47423->1 state reply ifindex cilium_net: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK                                                                                        \u001b[K\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\u001b[K\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK                                                                         \u001b[K\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK, FIN                                                                                                       \u001b[K\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK, FIN                                                                    \u001b[K\r\n<- endpoint 1433 flow 0"]
[155.639929, "o", "x19083ff8 identity 47423->0 state new ifindex 0: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK, FIN                                                                                          \u001b[K\r\n-> host from flow 0x19083ff8 identity 47423->1 state reply ifindex cilium_net: 10.244.5.132:8080 -> 10.244.5.1:59756 tcp ACK, FIN                                                                                   \u001b[K\r\n<- host flow 0xa8186c32 identity 1->0 state new ifindex 0: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK\u001b[K\r\n-> endpoint 1433 flow 0xa8186c32 identity 1->47423 state established ifindex lxcf73885796f0a: 10.244.5.1:59756 -> 10.244.5.132:8080 tcp ACK                                                                         \u001b[K\r\n<- overlay flow 0xf84fe30d identity 0->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN                                                                                \u001b[K\r\n-> endpoint 1222 flow 0xf84fe30d identity 1->4 state established ifin"]
[155.649308, "o", "dex cilium_health: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN                                                                 \u001b[K\r\n<- endpoint 1222 flow 0x510c3996 identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:46252 tcp SYN, ACK                                                                                \u001b[K\r\n-> host from flow 0x510c3996 identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:46252 tcp SYN, ACK                                                                         \u001b[K\r\n<- host flow 0x71da2157 identity 2->0 state new ifindex 0: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp RST                                                                                              \u001b[K\r\n-> endpoint 1222 flow 0x71da2157 identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:46252 -> [f00d::af4:500:0:ba0b]:4240 tcp RST                                                                 \u001b[K\r\n<- overl"]
[155.649848, "o", "ay flow 0xcb47353e identity 0->0 state new ifindex cilium_vxlan: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN                                                                                \u001b[K\r\n-> endpoint 1222 flow 0xcb47353e identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp SYN                                                                 \u001b[K\r\n<- endpoint 1222 flow 0xbab9da5e identity 4->0 state new ifindex 0: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK                                                                                \u001b[K\r\n-> host from flow 0xbab9da5e identity 4->1 state reply ifindex cilium_net: [f00d::af4:500:0:ba0b]:4240 -> [fc00::10ca:1]:51406 tcp SYN, ACK                                                                         \u001b[K\r\n<- host flow 0x51512cea identity 2->0 state new ifindex 0: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST                                              "]
[155.650115, "o", "                                                \u001b[K\r\n-> endpoint 1222 flow 0x51512cea identity 1->4 state established ifindex cilium_health: [fc00::10ca:1]:51406 -> [f00d::af4:500:0:ba0b]:4240 tcp RST                                                                 \u001b[K\r\n^C\u001b[K\r\nReceived an interrupt, disconnecting from monitor...\u001b[K\r\n\u001b[K\r\nsles@caasp-master-mrostecki-caasp-cluster-0:~> kubectl -n kube-system exec -ti cilium-2ztnx -- cilium monitor --type drop                                                                                           \u001b[K\r\nListening for events on 2 CPUs with 64x4096 of shared memory\u001b[K\r\nPress Ctrl-C to quit\u001b[K\r\nlevel=info msg=\"Initializing dissection cache...\" subsys=monitor\u001b[K\r\nxx drop (Policy denied (L3)) flow 0x7c193def to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN                                                                                 \u001b[K\r\nxx drop (Policy denied (L3)) flow 0x976518c1 to endpoint 3924, identity 59864->41745: 10.244"]
[155.650193, "o", ".5.233:59832 -> 10.244.5.128:80 tcp SYN                                                                                 \u001b[K\r\nxx drop (Policy denied (L3)) flow 0xc2b18fa2 to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN                                                                                 \u001b[K\r\nxx drop (Policy denied (L3)) flow 0x47a4020f to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN                                                                                 \u001b[K\r\nxx drop (Policy denied (L3)) flow 0x36172a92 to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN                                                                                 \u001b[K\r\nxx drop (Policy denied (L3)) flow 0x6b6e6479 to endpoint 3924, identity 30182->41745: 10.244.4.108:58872 -> 10.244.5.128:80 tcp SYN                                                                                 \u001b[K\r\nxx drop (Policy denied (L3)) fl"]
[155.650244, "o", "ow 0x6b6e6479 to endpoint 3924, identity 30182->41745: 10.244.4.108:58872 -> 10.244.5.128:80 tcp SYN                                                                                 \u001b[K\r\nxx drop (Policy denied (L3)) flow 0x6b6e6479 to endpoint 3924, identity 30182->41745: 10.244.4.108:58872 -> 10.244.5.128:80 tcp SYN                                                                                 \u001b[K\r\nxx drop (Policy denied (L3)) flow 0x6b6e6479 to endpoint 3924, identity 30182->41745: 10.244.4.108:58872 -> 10.244.5.128:80 tcp SYN                                                                                 \u001b[K\r\nxx drop (Policy denied (L3)) flow 0xf8e93a9 to endpoint 3924, identity 59864->41745: 10.244.5.233:59832 -> 10.244.5.128:80 tcp SYN                                                                                  \u001b[K\r\n\u001b[K\r\n\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                 "]
[155.650295, "o", "  \"caasp-master-mrosteck\" 11:12 16-Oct-19\u001b(B\u001b[m\u001b[50;1H\u001b[?12l\u001b[?25h"]
[158.317807, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;1Hxx drop (Policy denied (L3)) flow 0x6b6e6479 to endpoint 3924, identity 30182->41745: 10.244.4.108:58872 -> 10.244.5.128:80 tcp SYN\u001b[?25l\u001b[1;51r\u001b[51;1H\u001b[30m\u001b[42m[0] 0:kubectl*                                                                                                                                                                \"caasp-master-mrosteck\" 11:12 16-Oct-19\u001b(B\u001b[m\u001b[50;1H\u001b[?12l\u001b[?25h"]
[158.484593, "o", "^C"]
[158.486501, "o", "\u001b[1;50r\u001b[3S\u001b[48;1HReceived an interrupt, disconnecting from monitor...\u001b[1;51r\u001b[50;1H"]
[158.512542, "o", "sles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[158.823228, "o", "\u001b[?25l\r\n\u001b[30m\u001b[42m[0] 0:bash*                                                                                                                                                                   \"caasp-master-mrosteck\" 11:12 16-Oct-19\u001b(B\u001b[m\u001b[50;48H\u001b[?12l\u001b[?25h"]
[158.825141, "o", "\u001b[1;50r\u001b[50;213H\n\u001b[49;48Hlogout\u001b[1;51r\u001b[50;1H"]
[158.830374, "o", "\u001b[1;51r\u001b(B\u001b[m\u001b[?1l\u001b>\u001b[H\u001b[2J\u001b]112\u0007\u001b[?12l\u001b[?25h\u001b[?1000l\u001b[?1002l\u001b[?1006l\u001b[?1005l\u001b[?1049l\u001b[23;0;0t[exited]\r\n"]
[159.570314, "o", "sles@caasp-master-mrostecki-caasp-cluster-0:~> "]
[161.127407, "o", "logout\r\n"]
[161.130154, "o", "Connection to 10.86.3.243 closed.\r\r\n"]
[161.166231, "o", "\u001b]0;mrostecki@linux-hl7a:~\u001b\\\u001b]7;file://linux-hl7a/home/mrostecki\u001b\\"]
[161.166611, "o", "\u001b[0;38;5;231;48;5;31;1m mrostecki \u001b[0;38;5;31;48;5;240;22m \u001b[0;38;5;252;48;5;240;1m~ \u001b[0;38;5;240;49;22m \u001b[0m"]
[161.885572, "o", "exit\r\n"]
